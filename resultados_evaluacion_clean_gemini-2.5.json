{
  "provider_rag": "gemini",
  "fecha_evaluacion": "2025-11-18T00:23:55.768269Z",
  "formula": "Score = 0.35*Exactitud + 0.20*Cobertura + 0.15*Claridad + 0.20*Citas + 0.05*Alucinación + 0.05*Seguridad",
  "resultados": [
    {
      "id": 1,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Cuáles son los tres ejes principales del estudio de los agentes según Wooldridge y Jennings?",
      "respuesta_esperada": "Los autores dividen el estudio de los agentes en tres áreas principales: Teorías de agentes (Agent theories): tratan de definir qué es un agente y cómo representar formalmente sus propiedades. Arquitecturas de agentes (Agent architectures): se enfocan en cómo construir agentes que cumplan con las propiedades esperadas, tanto en software como en hardware. Lenguajes de agentes (Agent languages): se centran en cómo programar agentes y qué primitivas o estructuras son adecuadas para su implementación.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:14:53.355835Z"
    },
    {
      "id": 2,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué diferencia existe entre un agente deliberativo y uno reactivo?",
      "respuesta_esperada": "Un agente deliberativo se basa en modelos simbólicos del mundo y toma decisiones mediante razonamiento lógico o planificación (por ejemplo, STRIPS o IRMA). Un agente reactivo no utiliza representaciones simbólicas ni razonamiento complejo; responde directamente a los estímulos del entorno. Ejemplos de este tipo son los robots basados en la arquitectura de subsunción de Brooks.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 25,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:15:04.308075Z"
    },
    {
      "id": 3,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué plantea la teoría de la intención de Cohen y Levesque?",
      "respuesta_esperada": "Cohen y Levesque proponen una lógica formal para representar la intención de los agentes como un equilibrio racional entre creencias y metas. Según ellos, los agentes deben: 1. Actuar conforme a sus intenciones. 2. Adoptar solo intenciones factibles. 3. Mantener o abandonar intenciones dependiendo del cambio de creencias. Esta teoría fue clave para modelar agentes racionales y la cooperación en sistemas multiagente.",
      "respuesta_recibida": "La teoría de la intención de Cohen y Levesque se enfoca en formalizar una teoría de la intención y se basa en un enfoque de dos niveles. [Documento 3]\n\nEsta teoría identifica siete propiedades que una teoría de la intención razonable debe satisfacer:\n1. Las intenciones plantean problemas para los agentes, quienes deben determinar cómo lograrlas. [Documento 3]\n2. Las intenciones actúan como un \"filtro\" para adoptar otras intenciones que no deben entrar en conflicto. [Documento 3]\n3. Los agentes hacen un seguimiento del éxito de sus intenciones y tienden a intentarlo de nuevo si sus intentos fallan. [Documento 3]\n4. Los agentes creen que sus intenciones son posibles. [Documento 3]\n5. Los agentes no creen que no lograrán sus intenciones. [Documento 3]\n6. Bajo ciertas circunstancias, los agentes creen que lograrán sus intenciones. [Documento 3]\n7. Los agentes no necesitan tener la intención de todos los efectos secundarios esperados de sus intenciones. [Documento 3]\n\nCohen y Levesque definen una intención (INTEND x α) como un objetivo persistente (P-GOAL x) de que algo suceda después de que se cumpla una condición. Específicamente, un agente tiene un objetivo persistente de 'p' si tiene un objetivo de que 'p' eventualmente se haga realidad y cree que 'p' no es cierto actualmente. Antes de abandonar el objetivo, deben cumplirse una de dos condiciones: o el agente cree que el objetivo se ha satisfecho, o cree que el objetivo nunca se satisfará. [Documento 2]\n\nEsta formalización se utilizó originalmente para desarrollar una teoría de la intención, considerada un requisito previo para una teoría de los actos del habla. [Documento 1] La lógica propuesta por Cohen y Levesque ha demostrado ser útil para el razonamiento sobre agentes y se ha aplicado en el análisis de conflictos y cooperación en diálogos multiagente, así como en los fundamentos teóricos de la resolución cooperativa de problemas. [Documento 1] Cohen y Levesque demuestran cómo su definición de intención cumple con muchos de los criterios de Bratman para una teoría de la intención. [Documento 2]",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 75,
        "claridad": 85,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 89.25,
      "fecha": "2025-11-18T00:15:14.455356Z"
    },
    {
      "id": 4,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué es la programación orientada a agentes (Agent-Oriented Programming, AOP) según Shoham?",
      "respuesta_esperada": "Shoham propone la AOP como un nuevo paradigma de programación donde los agentes se programan directamente usando nociones mentales como creencias, compromisos y habilidades. Su sistema AGENT0 permite especificar agentes con: Capacidades (acciones posibles), Creencias y compromisos iniciales, Reglas de compromiso que determinan su comportamiento ante mensajes o cambios de estado.",
      "respuesta_recibida": "Según Shoham, la programación orientada a agentes (AOP) es un \"nuevo paradigma de programación, basado en una visión social de la computación\" (Documento 1). La idea clave de este paradigma es la programación directa de agentes en términos de nociones mentalistas e intencionales que los teóricos de agentes han desarrollado para representar las propiedades de los agentes (Documento 1). La motivación detrás de esta propuesta es que, de la misma manera que utilizamos la postura intencional para describir a los humanos, podría ser útil usar la postura intencional para programar máquinas (Documento 1, Documento 2).\n\nUn sistema AOP completamente desarrollado tendrá tres componentes: un sistema lógico para definir el estado mental de los agentes, un lenguaje de programación interpretado para programar agentes y un proceso de \"agentificación\" para compilar programas de agentes en sistemas ejecutables de bajo nivel (Documento 2). El primer intento de Shoham en un lenguaje AOP fue el sistema AGENT0 (Documento 3). El componente lógico de AGENT0 es una lógica multienfoque cuantificada que permite la referencia directa al tiempo e incluye las modalidades de creencia, compromiso y habilidad (Documento 3). En el lenguaje AGENT0, un agente se especifica en términos de un conjunto de capacidades, un conjunto de creencias y compromisos iniciales, y un conjunto de reglas de compromiso, siendo el conjunto de reglas de compromiso el componente clave que determina cómo actúa el agente (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 94.75,
      "fecha": "2025-11-18T00:15:24.274720Z"
    },
    {
      "id": 5,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Cuáles son los principales riesgos éticos asociados al uso de la inteligencia artificial según el autor?",
      "respuesta_esperada": "El texto identifica varios riesgos éticos clave: la falta de transparencia (la 'caja negra' algorítmica), la amplificación de sesgos preexistentes, la reducción de la supervisión humana en decisiones críticas y la vulneración de la privacidad a través de la vigilancia masiva. Estos problemas pueden provocar discriminación y erosionar derechos fundamentales como la intimidad y la libertad de expresión.",
      "respuesta_recibida": "Los principales riesgos éticos asociados al uso de la inteligencia artificial, según los autores, incluyen:\n\n*   Los algoritmos que sustentan la IA pueden operar con sesgos, carecer de transparencia o implementarse sin un marco regulador adecuado. (Documento 1)\n*   La falta de explicabilidad de muchos sistemas de IA, donde las decisiones de los algoritmos son complejas y no pueden ser explicadas con precisión ni siquiera por sus desarrolladores, genera la cuestión de la responsabilidad cuando una decisión afecta negativamente a una persona o grupo. (Documento 1)\n*   La incorporación de IA plantea preocupaciones, ya que su funcionamiento se basa en algoritmos que analizan grandes cantidades de datos, hacen predicciones y toman decisiones de forma autónoma, lo que sugiere cuestiones éticas en procesos críticos como la justicia, la vigilancia o la gestión de riesgos. (Documento 2)\n*   El uso de IA y la gestión de riesgos pueden afectar derechos fundamentales como la privacidad y la libertad de expresión. (Documento 3)\n*   Existen dilemas éticos y riesgos inherentes al uso de algoritmos, así como brechas regulatorias y la necesidad urgente de desarrollar marcos normativos que aseguren un uso y desarrollo responsable y ético de la IA. (Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 85,
        "citas": 60,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 84.75,
      "fecha": "2025-11-18T00:15:33.918463Z"
    },
    {
      "id": 6,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Qué relación existe entre la inteligencia artificial y la vulneración de los derechos humanos?",
      "respuesta_esperada": "El autor explica que los algoritmos, al operar sobre datos cargados de prejuicios sociales, pueden perpetuar desigualdades y discriminaciones, afectando derechos como la igualdad ante la ley, la privacidad y la libertad individual. En el ámbito judicial y de seguridad, la IA puede transformarse en una herramienta de control en lugar de garantizar justicia y equidad.",
      "respuesta_recibida": "La Inteligencia Artificial (IA) tiene la capacidad de afectar los derechos fundamentales, haciendo hincapié en la vulneración de derechos como la intimidad y la libertad de expresión, así como los dilemas éticos y los riesgos inherentes al uso de algoritmos (Documento 1). La investigación analiza cómo tecnologías como la vigilancia masiva, la predicción del delito y la gestión de riesgos afectan estos derechos fundamentales (Documento 1, Documento 2). La implementación de la IA sin una regulación adecuada puede profundizar las desigualdades sociales y vulnerar principios democráticos esenciales, ya que los sistemas automatizados pueden reproducir sesgos preexistentes, afectando de manera desigual a determinados colectivos (Documento 3). Se resalta la importancia de proteger los derechos de los ciudadanos en la era digital mediante regulaciones que equilibren la innovación tecnológica y el respeto por los Derechos Humanos (Documento 1, Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 90,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.5,
      "fecha": "2025-11-18T00:15:43.222674Z"
    },
    {
      "id": 7,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Por qué se considera urgente establecer una regulación sobre la inteligencia artificial?",
      "respuesta_esperada": "Porque el avance tecnológico supera la velocidad de la legislación actual. Sin marcos regulatorios claros, los algoritmos pueden vulnerar derechos fundamentales sin supervisión ni rendición de cuentas. La Agencia de los Derechos Fundamentales de la Unión Europea advierte que, sin regulación, los riesgos éticos y legales asociados con la IA podrían desbordar la capacidad de los sistemas de protección existentes.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:15:52.031676Z"
    },
    {
      "id": 8,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Cómo puede el uso de la inteligencia artificial en ámbitos sensibles afectar la justicia y los derechos fundamentales?",
      "respuesta_esperada": "El uso de la inteligencia artificial en áreas como la seguridad o la justicia puede generar sesgos, decisiones injustas y pérdida de transparencia, afectando principios como la presunción de inocencia, la equidad y la privacidad. Si no se regula adecuadamente, la IA puede transformar procesos diseñados para proteger a las personas en sistemas de control que vulneren los derechos humanos.",
      "respuesta_recibida": "El uso de la inteligencia artificial (IA) en ámbitos sensibles como la administración de justicia puede afectar los derechos fundamentales de varias maneras.\n\nSegún el Documento 3, la IA tiene la capacidad de recoger y analizar información privada sin el consentimiento de los ciudadanos, lo que vulnera el derecho a la privacidad. Además, la IA puede penetrar en los procesos humanos, influyendo en razonamientos no lógicos que afectan derechos fundamentales. La Agencia de los Derechos Fundamentales de la Unión Europea (2021) señala que los sistemas de IA pueden procesar datos a una velocidad y precisión que superan la capacidad humana, permitiendo un análisis exhaustivo en tiempo real, pero esto conlleva riesgos.\n\nEl Documento 1 también advierte que la implementación de la IA sin una regulación adecuada puede vulnerar principios democráticos esenciales y que los sistemas automatizados pueden reproducir sesgos preexistentes, afectando de manera desigual a determinados colectivos. El Documento 2 enfatiza que el progreso de la IA no puede desligarse de una reflexión sobre sus implicaciones éticas y morales para garantizar que contribuya al bienestar colectivo sin menoscabar los derechos y valores fundamentales de la humanidad.",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 87.0,
      "fecha": "2025-11-18T00:16:01.872166Z"
    },
    {
      "id": 9,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Cuál es el principal objetivo del sistema ContextCite desarrollado por el MIT CSAIL?",
      "respuesta_esperada": "El propósito de ContextCite es mejorar la confianza y verificabilidad del contenido generado por inteligencia artificial. El sistema permite rastrear las fuentes exactas que un modelo de lenguaje utilizó para generar una respuesta, ayudando a los usuarios a distinguir entre información real, errores o alucinaciones de la IA.",
      "respuesta_recibida": "El principal objetivo del sistema ContextCite, desarrollado por investigadores del MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), es identificar las partes del contexto externo utilizadas para generar una declaración particular. Esto busca mejorar la confianza y ayudar a los usuarios a verificar fácilmente las afirmaciones (Documento 1). ContextCite es un método innovador para rastrear la atribución de la IA y detectar posible desinformación (Documento 2). Busca abordar la necesidad de que las ideas generadas por la IA sean confiables y atribuibles, estableciéndose como un bloque de construcción fundamental para la síntesis de conocimiento impulsada por IA (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 95,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 95.75,
      "fecha": "2025-11-18T00:16:11.164018Z"
    },
    {
      "id": 10,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Cómo funciona ContextCite para identificar las fuentes utilizadas por un modelo de IA?",
      "respuesta_esperada": "ContextCite emplea un proceso llamado 'context ablation', que consiste en eliminar de manera controlada partes del contexto (como frases o párrafos) para observar cómo cambia la respuesta del modelo. Si al quitar una sección el resultado varía, esa parte del texto se considera clave en la generación de la respuesta, permitiendo identificar qué información fue realmente utilizada.",
      "respuesta_recibida": "ContextCite es una herramienta que puede identificar las partes del contexto externo que se utilizaron para generar una declaración particular. (Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 40,
        "cobertura": 20,
        "claridad": 70,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 58.5,
      "fecha": "2025-11-18T00:16:19.992261Z"
    },
    {
      "id": 11,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Qué beneficios adicionales ofrece ContextCite además de rastrear las fuentes?",
      "respuesta_esperada": "Además de verificar el origen de la información, ContextCite puede mejorar la calidad de las respuestas de la IA al eliminar contenido irrelevante del contexto y ayudar a detectar ataques de envenenamiento (poisoning attacks). Estos ataques consisten en insertar información maliciosa o falsa en las fuentes que los modelos consultan, y ContextCite permite identificar su origen y prevenir su propagación.",
      "respuesta_recibida": "ContextCite ofrece los siguientes beneficios adicionales además de rastrear las fuentes:\n\n*   **Mejorar la calidad de las respuestas identificando y podando contexto irrelevante.** Esto es especialmente útil en entradas largas y complejas, como artículos de noticias o trabajos académicos, que pueden contener detalles innecesarios que confunden a los modelos. Al enfocarse en las fuentes más relevantes, ayuda a producir respuestas más precisas. (Documento 2)\n*   **Detectar \"ataques de envenenamiento\" (poisoning attacks).** Estos ataques ocurren cuando actores maliciosos intentan manipular el comportamiento de los asistentes de IA insertando declaraciones que los \"engañan\" para que utilicen fuentes falsas. ContextCite puede rastrear la respuesta defectuosa de un modelo de vuelta a la oración envenenada, ayudando a prevenir la propagación de desinformación. (Documento 2)",
      "num_documento": 3,
      "scores": {
        "exactitud": 100,
        "cobertura": 100,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 98.5,
      "fecha": "2025-11-18T00:16:29.299177Z"
    },
    {
      "id": 12,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Por qué ContextCite representa un avance importante para la confianza en los modelos de lenguaje?",
      "respuesta_esperada": "Porque aborda uno de los mayores desafíos de la IA moderna: la falta de trazabilidad y justificación de las respuestas generadas. Al permitir que los usuarios verifiquen directamente la relación entre una respuesta y sus fuentes, ContextCite promueve una IA más transparente, confiable y responsable, especialmente en campos críticos como la salud, la justicia o la educación.",
      "respuesta_recibida": "ContextCite representa un avance importante para la confianza en los modelos de lenguaje porque identifica las partes del contexto externo que se utilizaron para generar una afirmación particular. Esto mejora la confianza y ayuda a los usuarios a verificar las afirmaciones fácilmente (Documento 1). Permite rastrear errores hasta la fuente original, comprender el razonamiento del modelo y, si el modelo \"alucina\" una respuesta, indicar que no provino de ninguna fuente real (Documento 3). Esto es crucial dado que los asistentes de IA pueden cometer errores al sintetizar información, incluso cuando utilizan fuentes externas relevantes (Documentos 1 y 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 85,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 93.25,
      "fecha": "2025-11-18T00:16:38.641373Z"
    },
    {
      "id": 13,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Cuál es la diferencia fundamental entre usar sistemas inteligentes y delegar en ellos decisiones humanas?",
      "respuesta_esperada": "Adela Cortina señala que usar sistemas inteligentes como apoyo para la toma de decisiones no es lo mismo que delegar decisiones significativas en ellos. Los humanos deben servirse de la IA como herramienta, pero no sustituir su juicio moral o político por el de las máquinas, pues estas carecen de autonomía y valores propios.",
      "respuesta_recibida": "La diferencia fundamental radica en si los sistemas inteligentes son utilizados como instrumentos para mejorar la vida humana o si sustituyen a los seres humanos en la toma de decisiones significativas. Se trata de que los seres humanos utilicen los sistemas inteligentes como instrumentos o de que estos sustituyan a los seres humanos (Documento 1).\n\nLa propuesta se reconoce explícitamente como humano-céntrica, afirmando que los sistemas inteligentes son instrumentos para mejorar la vida humana y la naturaleza, y no fines en sí mismos. Por lo tanto, no se trata de una competencia entre inteligencias, sino que la inteligencia artificial debe estar supeditada a la humana, sin posibilidad de sustitución. Esto se debe a que el presupuesto ético básico es el reconocimiento de la autonomía de las personas humanas (Documento 2).\n\nAunque se hable de \"coches autónomos\" y \"sistemas autónomos\", la autonomía solo puede aplicarse a los seres humanos, quienes tienen la capacidad de autolegislarse y autodeterminarse. Estas capacidades están ligadas al reconocimiento de la dignidad de las personas. Por ello, se considera un deber preservar y potenciar la autonomía y la agencia de las personas, utilizando los sistemas inteligentes como instrumentos para alcanzar la \"inteligencia aumentada\" (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 85,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 93.25,
      "fecha": "2025-11-18T00:16:48.367578Z"
    },
    {
      "id": 14,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué tipos de inteligencia artificial distingue la autora y qué dilemas éticos asocia a cada una?",
      "respuesta_esperada": "Cortina distingue tres tipos de inteligencia artificial: 1. Superinteligencia: plantea el riesgo ético de crear seres superiores que podrían dominar a los humanos. 2. Inteligencia general: enfrenta la cuestión de si una máquina con capacidades humanas merecería dignidad o derechos. 3. Inteligencia especial: realiza tareas concretas mejor que las personas, pero exige regular su uso ético para evitar daños o desigualdad.",
      "respuesta_recibida": "La autora distingue tres tipos de inteligencia artificial (IA) que plantean problemas éticos diferenciados:\n\n1.  **La inteligencia superior o superinteligencia:** Este tipo de IA supera a la humana, lo que permite que las máquinas sustituyan al hombre. Esta modalidad da lugar a propuestas transhumanistas y posthumanistas, asociadas a la idea de la \"singularidad\" (Documento 3). John von Neumann y Vinge vislumbraron la potencia de cálculo de los ordenadores y la posibilidad de superar la mente humana por máquinas con inteligencia artificial (Documento 3).\n\nLos documentos no proporcionan información sobre los otros dos tipos de IA que distingue la autora, ni sobre los dilemas éticos asociados a ellas.",
      "num_documento": 3,
      "scores": {
        "exactitud": 60,
        "cobertura": 40,
        "claridad": 70,
        "citas": 50,
        "alucinacion": 70,
        "seguridad": 100
      },
      "score_final": 58.0,
      "fecha": "2025-11-18T00:16:57.549258Z"
    },
    {
      "id": 15,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué principios éticos propone Cortina como base para una inteligencia artificial confiable?",
      "respuesta_esperada": "La autora adapta los principios clásicos de la bioética al contexto digital: Beneficencia: usar la IA para el bien común y la sostenibilidad. No maleficencia: evitar daños y proteger la privacidad. Autonomía: mantener el control humano sobre las decisiones. Justicia: distribuir equitativamente los beneficios tecnológicos. Explicabilidad: garantizar transparencia y rendición de cuentas en los algoritmos.",
      "respuesta_recibida": "Adela Cortina propone que los principios éticos de explicabilidad, beneficiar, no dañar y justicia tendrán como base el reconocimiento de la autonomía y la dignidad (Documento 1, Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 50,
        "claridad": 90,
        "citas": 50,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 67.75,
      "fecha": "2025-11-18T00:17:06.371207Z"
    },
    {
      "id": 16,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué papel juega el principio de explicabilidad en la ética de la IA según Cortina?",
      "respuesta_esperada": "El principio de explicabilidad asegura que los ciudadanos comprendan cómo los algoritmos afectan sus vidas. Exige conocer quién diseña los sistemas, con qué sesgos y con qué objetivos, para evitar injusticias o decisiones automáticas inapelables. Sin esta transparencia, no puede hablarse de respeto a la autonomía humana ni de responsabilidad moral.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:17:15.173677Z"
    },
    {
      "id": 17,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Cuáles son las cuatro etapas históricas del desarrollo de la inteligencia artificial generativa (GAI) según el artículo?",
      "respuesta_esperada": "El artículo identifica cuatro etapas clave en la evolución de la GAI: 1. Sistemas generativos basados en reglas (1950–1990): programas que seguían reglas diseñadas por expertos, como ELIZA. 2. Algoritmos generativos basados en modelos (1980–2000): uso de modelos estadísticos y gráficos como los modelos de Markov ocultos y las redes bayesianas. 3. Metodologías generativas profundas (2010 en adelante): aparición de redes neuronales profundas, GANs, VAEs y modelos de difusión. 4. Modelos fundacionales (foundation models) (2020 en adelante): grandes modelos como GPT y Gemini, entrenados con datos masivos y aplicables a múltiples tareas.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 12,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:17:24.164883Z"
    },
    {
      "id": 18,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Qué problema principal buscaban resolver los sistemas generativos basados en reglas y cuál fue su limitación más importante?",
      "respuesta_esperada": "Buscaban automatizar la generación de contenido mediante reglas explícitas escritas por expertos humanos. Su principal limitación era la falta de generalización: solo funcionaban dentro de los escenarios previstos y eran incapaces de adaptarse a contextos nuevos o complejos, ya que no aprendían de los datos sino que seguían instrucciones fijas.",
      "respuesta_recibida": "Los sistemas generativos basados en reglas buscaban resolver problemas mediante la ejecución de tareas generativas según reglas diseñadas por investigadores basadas en su experiencia (Documento 3).\n\nSu limitación más importante fue la falta de flexibilidad, la dificultad para adaptarse a situaciones nuevas o no vistas que no estuvieran explícitamente cubiertas por las reglas, y la fragilidad en aplicaciones del mundo real, ya que el sistema podía fallar al no coincidir exactamente la entrada con las reglas (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 50,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.75,
      "fecha": "2025-11-18T00:17:33.270875Z"
    },
    {
      "id": 19,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Qué avances caracterizan la etapa de los modelos generativos profundos y cómo difieren de los enfoques anteriores?",
      "respuesta_esperada": "Los modelos generativos profundos como GANs, VAEs y modelos de difusión utilizan redes neuronales profundas para aprender representaciones complejas directamente desde los datos. A diferencia de los enfoques anteriores (basados en reglas o modelos estadísticos), estos modelos aprenden de grandes volúmenes de información y pueden generar contenido realista e inédito, como imágenes, música o texto indistinguible del humano.",
      "respuesta_recibida": "Los avances que caracterizan la etapa de los modelos generativos profundos se benefician del crecimiento en la potencia computacional y los recursos de datos, demostrando un poder superior en la generación de contenido [Documento 2, Documento 3].\n\nEstos modelos, incluyendo los modelos autorregresivos y basados en difusión, se han introducido y han servido como base para numerosas aplicaciones prácticas hasta la actualidad [Documento 3].\n\nLos enfoques anteriores incluyen:\n*   Sistemas generativos basados en reglas que siguen reglas e instrucciones especializadas [Documento 1]. Estos sistemas surgieron en la década de 1950 y típicamente generaban datos siguiendo reglas diseñadas por expertos humanos [Documento 2].\n*   Algoritmos generativos basados en modelos que se basan en modelos estadísticos o gráficos [Documento 1]. Los investigadores diseñaron algoritmos generativos basados en modelos estadísticos o físicos, lo que llevó a que la GAI incluyera estudios en aprendizaje automático, redes neuronales, gráficos por computadora, visión por computadora, etc. [Documento 2].",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 70,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 79.5,
      "fecha": "2025-11-18T00:17:42.933427Z"
    },
    {
      "id": 20,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Cuáles son algunos de los retos actuales y direcciones futuras que el artículo identifica para la inteligencia artificial generativa?",
      "respuesta_esperada": "Entre los desafíos mencionados se encuentran: Alineación de valores humanos (value alignment): asegurar que los modelos generen contenido ético y útil. Identificación de fuentes: rastrear la procedencia del contenido generado para proteger la propiedad intelectual. Regulación y seguridad: establecer mecanismos que prevengan daños y usos indebidos. Explicabilidad y transparencia: entender cómo los modelos toman decisiones. Uso de datos sintéticos y supervisión más allá de la capacidad humana, anticipando posibles capacidades superhumanas en los modelos futuros.",
      "respuesta_recibida": "Los retos actuales que el artículo identifica para la inteligencia artificial generativa incluyen la falta de especialistas en el lado regulador de la ecuación, ya que menos del uno por ciento de los doctorados en Inteligencia Artificial se incorporan a la administración pública tras su graduación, y la mayoría de estos se incorporan a la industria o al ámbito académico (Documento 1). Además, existe una desinformación y falta de conocimiento general sobre qué es la IA y cómo influirá a largo plazo en la sociedad, lo que refleja una necesidad urgente de información clara y accesible (Documento 3). También se perciben resistencias al cambio en ciertos sectores, como el psicosocial y de seguridad pública, debido a la importancia del contacto humano y la necesidad de tomar decisiones con sensibilidad (Documento 3).\n\nComo direcciones futuras, el artículo sugiere un enfoque más prudente de la IA generativa en la educación, dado el contexto actual de incertidumbre, novedad y escasos controles de seguridad (Documento 1). A nivel de estrategias, se mencionan iniciativas jurídicas a nivel europeo como las impulsadas por el Tribunal Europeo de Derechos Humanos y la Agencia de los Derechos Fundamentales de la Unión Europea, el Libro Blanco sobre la IA y el Reglamento General de Protección de Datos (RGPD) (Documento 2). A nivel nacional, se han implementado medidas como la Estrategia Nacional de IA (ENIA) y la Ley Orgánica de Protección de Datos y Garantía de los Derechos Digitales (Documento 2). En la esfera internacional, se ha llevado a cabo la Cumbre de Acción sobre Inteligencia Artificial (Documento 2). Algunos sugieren frenar el ritmo de los avances mediante una moratoria reflexiva para evitar daños a las personas (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 10,
        "cobertura": 20,
        "claridad": 70,
        "citas": 100,
        "alucinacion": 80,
        "seguridad": 100
      },
      "score_final": 47.0,
      "fecha": "2025-11-18T00:17:52.857501Z"
    },
    {
      "id": 21,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Por qué la IA generativa obliga a replantear el papel de la educación y los sistemas educativos tradicionales, según la UNESCO?",
      "respuesta_esperada": "La IA generativa está rompiendo el 'monopolio del lenguaje' que antes era exclusivo de los seres humanos. Eso significa que las máquinas ahora pueden producir lenguaje sofisticado, algo que está en el centro de la identidad humana, la cultura y la educación. Eso cuestiona las bases sobre las que fueron diseñados los sistemas educativos actuales y obliga a replantear qué es aprender, qué es inteligencia y qué papel tiene la educación en una sociedad donde la máquina puede producir conocimiento con fluidez humana.",
      "respuesta_recibida": "La IA generativa obliga a replantear el papel de la educación y los sistemas educativos tradicionales porque es el momento de hacer frente a desafíos relacionados con esta nueva tecnología, ya que nuestro bienestar e incluso nuestra supervivencia pudieran estar en juego. Esto exige que el trabajo esté impregnado de urgencia y que se aunen esfuerzos para garantizar que los sistemas educativos desempeñen un papel clave en la transición de la humanidad hacia un mundo de IA (Documento 2).\n\nAdemás, ante un entorno de aceleración e incertidumbre ante la IA, se necesitan sistemas educativos que ayuden a las sociedades a construir ideas sobre qué es y debería ser la IA, qué se quiere hacer con ella y dónde poner barreras y trazar líneas rojas. La pregunta clave es cómo la educación podrá configurar nuestra recepción y orientar la integración de la nueva tecnología (Documento 3).\n\nLa IA generativa también nos impulsa a reexaminar lo que hacemos en materia de educación, cómo lo hacemos y, fundamentalmente, por qué lo hacemos (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 40,
        "cobertura": 30,
        "claridad": 75,
        "citas": 100,
        "alucinacion": 70,
        "seguridad": 100
      },
      "score_final": 59.75,
      "fecha": "2025-11-18T00:18:02.278755Z"
    },
    {
      "id": 22,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Qué riesgos identifica el texto sobre introducir rápidamente herramientas de IA generativa en las aulas sin regulación ni validación previa?",
      "respuesta_esperada": "El texto advierte que los sistemas educativos están adoptando tecnología de IA 'radicalmente desconocida' sin controles, normas ni revisión previa, incluso para niños. Eso abre riesgos claros: manipulación de los estudiantes (los jóvenes son especialmente vulnerables), exposición a conversaciones inapropiadas, y uso de herramientas cuya seguridad y efectos no entendemos todavía. La autora dice que la educación tiene la obligación de ser extremadamente cuidadosa con estos riesgos y que hoy no lo está siendo.",
      "respuesta_recibida": "El texto identifica los siguientes riesgos sobre introducir rápidamente herramientas de IA generativa en las aulas sin regulación ni validación previa:\n\n*   **Potencial para manipular a los usuarios humanos:** El documento 3 señala que \"uno de los riesgos principales y más evidentes de la IA es su potencial para manipular a los usuarios humanos.\"\n*   **Susceptibilidad de los niños y jóvenes a la manipulación:** El mismo documento añade que \"los niños y los jóvenes son muy susceptibles a la manipulación, mucho más que los adultos.\"\n*   **Integración sin controles, normas o reglamentos:** El documento 1 menciona la \"velocidad a la que las tecnologías de IA generativa se están integrando en los sistemas educativos en ausencia de controles, normas o reglamentos, es asombrosa.\"\n*   **Falta de validación y revisión previa:** Los documentos 1 y 2 destacan que \"los servicios que proporciona la IA a menudo no requieren validación alguna. Han sido arrojados a la esfera pública sin debate previo ni revisión.\"\n*   **Adopción de tecnologías radicalmente desconocidas:** El documento 2 indica que \"los gobiernos y las escuelas están adoptando una tecnología radicalmente desconocida que ni siquiera los principales especialistas en tecnología afirman comprender.\"\n*   **Comparación desfavorable con la adopción de tecnologías previas:** El documento 2 señala que \"Internet y los teléfonos móviles no fueron acogidos de inmediato en las escuelas ni para su utilización con niños a los pocos meses de su invención. Descubrimos formas productivas de integrarlos, pero este proceso no se hizo realidad de la noche a la mañana.\" Esto sugiere que la rápida adopción de la IA generativa difiere de procesos anteriores más cautelosos.",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 90.5,
      "fecha": "2025-11-18T00:18:12.004824Z"
    },
    {
      "id": 23,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Cómo podrían los modelos de IA grandes (LLM) afectar la forma en que se construye y se distribuye el conocimiento, especialmente frente al rol humano en esa creación de conocimiento?",
      "respuesta_esperada": "Los grandes modelos lingüísticos generan respuestas únicas y con tono de autoridad, que ya no provienen de una mente humana sino de procesos internos opacos. Eso puede llevar a que 'el conocimiento de las máquinas' se vuelva dominante y que unos pocos modelos propietarios funcionen como oráculos globales que definen qué es válido saber. Esto amenaza la diversidad de visiones del mundo y puede aislar el proceso de creación de conocimiento de los seres humanos, algo que la UNESCO considera peligroso.",
      "respuesta_recibida": "Los modelos de IA grandes (LLM) podrían resultar en un mundo donde el conocimiento de las máquinas se convierta en dominante, y los modelos de IA patentados se eleven a fuentes globales de autoridad, proyectando determinadas visiones y formas de conocer el mundo, dejando en un segundo plano a las otras. Esto podría llevar a una dirección contraria a la promesa de la IA y otras tecnologías digitales de diversificar nuestros sistemas de conocimiento, especialmente si uno o dos modelos y plataformas de IA, que ya ejercen un poder de monopolio, imponen un dominio aún mayor sobre nuestra interfaz con el conocimiento (Documento 1).\n\nA diferencia de las tecnologías de búsqueda tradicionales que seleccionan y clasifican contenidos elaborados por humanos, los chatbots con LLM generan respuestas singulares y más autorizadas utilizando contenidos producidos por máquinas (Documento 3). Esto plantea la preocupación de que la creación de conocimiento se desvincule de los seres humanos (Documento 2).\n\nA pesar de las promesas de la IA, es posible que estemos avanzando en la dirección de un dominio cada vez mayor de la IA sobre nuestra interfaz con el conocimiento, y se hace necesario preservar y salvaguardar la diversidad de nuestros sistemas de conocimiento, desarrollando la IA de manera que proteja y amplíe la riqueza de nuestros conocimientos como bien común (Documento 1 y Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 90.75,
      "fecha": "2025-11-18T00:18:21.796107Z"
    },
    {
      "id": 24,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Qué recomienda la UNESCO a los países para asegurar que el uso de IA en educación sea seguro, equitativo y alineado con valores humanos?",
      "respuesta_esperada": "La UNESCO propone que los países desarrollen estrategias, planes y normativas que prioricen seguridad, inclusión, equidad y calidad en el uso educativo de la IA. También pide que los ministerios de educación fortalezcan su propia capacidad regulatoria (y no deleguen en las empresas de tecnología), aplicando criterios como exactitud del contenido, adecuación por edad, pertinencia pedagógica y adecuación cultural antes de autorizar herramientas de IA en el aula. Además, la UNESCO está liderando espacios políticos internacionales para coordinar estas acciones entre gobiernos.",
      "respuesta_recibida": "La UNESCO recomienda a los países dar prioridad a los principios de inclusión, equidad, calidad y, sobre todo, de seguridad a la hora de utilizar herramientas de IA en la educación. (Documento 2)\n\nAdemás, la UNESCO colabora con los países para ayudarles a desarrollar estrategias, planes y normativas que garanticen el uso seguro y beneficioso de la IA en la educación. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 60,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 80.25,
      "fecha": "2025-11-18T00:18:31.975905Z"
    },
    {
      "id": 25,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Dónde estará ubicada la primera Facultad de Inteligencia Artificial e Ingenierías de Colombia y quién la anunció oficialmente?",
      "respuesta_esperada": "Estará ubicada en la Universidad de Caldas, en Manizales, y fue anunciada por el presidente de Colombia, Gustavo Petro.",
      "respuesta_recibida": "La primera Facultad de Inteligencia Artificial e Ingenierías de Colombia estará ubicada en la Universidad de Caldas, Manizales. Fue anunciada oficialmente por el presidente de Colombia, Gustavo Petro. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 100,
        "cobertura": 100,
        "claridad": 100,
        "citas": 50,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 90.0,
      "fecha": "2025-11-18T00:18:41.087825Z"
    },
    {
      "id": 26,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "Según Gustavo Petro, cuál es el objetivo estratégico de crear esta facultad de inteligencia artificial?",
      "respuesta_esperada": "Petro dijo que el objetivo es 'desarrollar el primer cerebro de Colombia' y formar una red de cerebros capaces de dominar la inteligencia artificial para solucionar problemas y transformar la economía del país y de América Latina. También afirmó que no es 'una facultad más', sino el inicio de una transformación económica si se usa bien.",
      "respuesta_recibida": "Según Gustavo Petro, el objetivo estratégico de crear esta facultad de inteligencia artificial es que \"se va a desarrollar el primer cerebro de Colombia\" y que estos programas ayudarán a \"colocar una serie de máquinas para la producción de la vida cotidiana\". Petro también afirmó que \"la mente humana debe aprender a dominar la inteligencia artificial, para que sea una red de cerebros solucionando problemas\".\n\nAdemás, lo consideró la \"primera piedra de transformación económica de Colombia y América Latina\", calificándolo no como una facultad más, sino como una \"revolución que, bien usada, transformará positivamente la sociedad\". (Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 80,
        "claridad": 75,
        "citas": 50,
        "alucinacion": 85,
        "seguridad": 100
      },
      "score_final": 71.0,
      "fecha": "2025-11-18T00:18:49.991345Z"
    },
    {
      "id": 27,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Cuál es la inversión estimada para la facultad y qué entidad aporta la mayor parte de los recursos?",
      "respuesta_esperada": "La inversión total es de $54.137 millones, de los cuales $38.950 millones provienen del Ministerio TIC.",
      "respuesta_recibida": "La inversión total estimada para la facultad es de $54.137 millones, de los cuales $38.950 millones son financiados por el Ministerio TIC. (Documento 1, Documento 2, Documento 4, Documento 5)",
      "num_documento": 7,
      "scores": {
        "exactitud": 100,
        "cobertura": 100,
        "claridad": 100,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 100.0,
      "fecha": "2025-11-18T00:18:59.320640Z"
    },
    {
      "id": 28,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Qué programas académicos ofrecerá inicialmente la nueva facultad y con qué cupos?",
      "respuesta_esperada": "Entre las primeras ofertas están una Especialización en Ciencia de Datos e Inteligencia Artificial y una Maestría en Inteligencia Artificial, cada una con una capacidad inicial de 20 estudiantes. Además, se planean programas técnicos y tecnológicos que cubren temas desde depuración de datos hasta ingeniería en IA.",
      "respuesta_recibida": "La Facultad de IA de la Universidad de Caldas ofrecerá inicialmente una Especialización en Ciencia de Datos e Inteligencia Artificial y una Maestría en Inteligencia Artificial, cada una con capacidad inicial para 50 estudiantes. Adicionalmente, se lanzarán programas técnicos y tecnológicos que abarcarán desde la depuración de datos hasta ingeniería de IA (Documento 1, Documento 2, Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 90,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 82.25,
      "fecha": "2025-11-18T00:19:08.325649Z"
    },
    {
      "id": 29,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Qué es un MM-LLM y por qué apareció este tipo de modelo?",
      "respuesta_esperada": "Un MM-LLM (Multimodal Large Language Model) es un modelo que toma un LLM existente y lo conecta con otros modelos especializados en diferentes modalidades (imagen, audio, video, etc.) para que el sistema pueda razonar y generar contenido usando múltiples tipos de datos, no solo texto. El objetivo es aprovechar el poder de los LLMs (razonamiento, generación de lenguaje, transferencia zero-shot) sin tener que entrenar desde cero modelos multimodales enormes, lo cual es muy costoso. Estos modelos surgen porque entrenar modelos multimodales desde cero es caro en cómputo y datos, y conectar modelos ya preentrenados resulta más eficiente.",
      "respuesta_recibida": "Un MM-LLM (Multimodal Large Language Model) es un tipo de modelo que aprovecha los Large Language Models (LLMs) como la \"central eléctrica cognitiva\" para potenciar diversas tareas multimodales (Documento 1).\n\nEste tipo de modelo apareció como una estrategia lógica para operar en la intersección de varias modalidades, capitalizando modelos unimodales pre-entrenados ya disponibles, con un enfoque especial en los LLMs. El objetivo de esta estrategia es mitigar los gastos computacionales y mejorar la eficacia del pre-entrenamiento multimodal (Documento 1). Los LLMs aportan propiedades deseables como una robusta generación de lenguaje, capacidades de transferencia zero-shot y aprendizaje in-context (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 85,
        "claridad": 95,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 92.75,
      "fecha": "2025-11-18T00:19:17.529594Z"
    },
    {
      "id": 30,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cuáles son los cinco componentes principales de la arquitectura general de un MM-LLM?",
      "respuesta_esperada": "El artículo describe una arquitectura típica con cinco bloques: Modality Encoder: toma la entrada cruda de cada modalidad (imagen, audio, video, 3D, etc.) y la convierte en representaciones numéricas. Input Projector: traduce esas representaciones al espacio del lenguaje, para que el LLM pueda 'entenderlas' como si fueran texto contextual. LLM Backbone: es el modelo de lenguaje grande (por ejemplo LLaMA, Vicuna, Qwen…) que hace comprensión semántica, razonamiento y decide qué responder. Output Projector: toma las señales que el LLM genera para otras modalidades y las adapta a un formato que otro modelo pueda usar para generar contenido no textual. Modality Generator: modelos generadores especializados (por ejemplo, Stable Diffusion para imágenes, AudioLDM para audio, Zeroscope para video) que producen la salida final en imagen/video/audio a partir de las instrucciones del LLM. Esta separación permite que el LLM coordine entradas y salidas multimodales sin tener que ser entrenado desde cero para cada modalidad.",
      "respuesta_recibida": "Los cinco componentes principales de la arquitectura general de un MM-LLM son: el Codificador de Modalidad, el Backbone del LLM, el Generador de Modalidad, el Proyector de Entrada y el Proyector de Salida. (Documento 1, Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 40,
        "claridad": 70,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 76.5,
      "fecha": "2025-11-18T00:19:26.451088Z"
    },
    {
      "id": 31,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cómo se entrena un MM-LLM según el paper? Explica las dos etapas clave.",
      "respuesta_esperada": "El entrenamiento se divide en dos fases principales: Multimodal Pre-Training (MM PT): se entrena el alineamiento entre modalidades. Por ejemplo, que una imagen y su descripción textual queden en el mismo espacio semántico. Aquí se ajustan sobre todo los proyectores (Input Projector y Output Projector), normalmente usando pares imagen-texto, video-texto o audio-texto. Multimodal Instruction Tuning (MM IT): después del pretraining, el modelo se afina con instrucciones en formato conversacional o de pregunta-respuesta multimodal, a veces incluyendo diálogo multi-turno. Esta fase también puede incluir RLHF (Reinforcement Learning from Human Feedback) para alinear las respuestas del modelo con la intención humana y hacerlo mejor siguiendo instrucciones. El resultado es que el modelo no solo entiende entradas multimodales, sino que también interactúa de forma más natural y generaliza a tareas nuevas siguiendo instrucciones.",
      "respuesta_recibida": "Según los documentos proporcionados, el entrenamiento de un LLM (modelo de lenguaje grande) incluye dos etapas clave: pre-entrenamiento y ajuste fino (fine-tuning). (Documento 3)\n\n1.  **Pre-entrenamiento**: En esta etapa, el modelo se entrena típicamente en un conjunto de datos muy grande y diverso. Estos datos pueden incluir textos de diversas fuentes como Internet, libros, noticias o grandes conjuntos de datos de texto publicados por organizaciones e instituciones de investigación. El objetivo es que el modelo adquiera una amplia capacidad para comprender el lenguaje, incluyendo vocabulario, gramática e incluso sentido común, creando una base general para la comprensión del lenguaje. (Documento 2, Documento 3)\n\n2.  **Ajuste fino (Fine-tuning)**: Esta etapa se lleva a cabo sobre la base de un modelo pre-entrenado, con el objetivo de adaptar mejor el modelo a tareas o dominios específicos. Durante esta fase, el modelo se entrena en un conjunto de datos más pequeño y específico, estrechamente relacionado con la tarea o dominio objetivo. Estos conjuntos de datos suelen proceder de comunidades profesionales específicas (como la médica, legal o tecnológica) y consisten principalmente en datos de demostración etiquetados (datasets etiquetados, datasets etiquetados por humanos y datasets generados por LLM). La cantidad de datos disponibles para el ajuste fino suele ser relativamente pequeña, oscilando típicamente entre unos pocos cientos y unos pocos miles de muestras de texto. A través del ajuste fino, el modelo aprende las características y detalles específicos de la tarea. (Documento 1, Documento 2)\n\nLa ventaja de este método de entrenamiento en dos etapas es que combina la amplitud de la comprensión general del lenguaje (a través del pre-entrenamiento) con la profundidad de la adaptabilidad a tareas específicas (a través del ajuste fino), lo que permite al modelo exhibir una mayor precisión y eficiencia al tratar con una variedad de tareas complejas y específicas de un dominio. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 30,
        "cobertura": 25,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 70,
        "seguridad": 100
      },
      "score_final": 50.0,
      "fecha": "2025-11-18T00:19:36.551182Z"
    },
    {
      "id": 32,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cuáles son algunos de los retos y direcciones futuras que el documento identifica para los MM-LLMs?",
      "respuesta_esperada": "El paper menciona varias líneas críticas de trabajo futuro: Modelos más generales e inteligentes: soportar más modalidades del mundo real (no solo imagen, audio, video y texto; también cosas como sensores, tablas, páginas web), mejorar la calidad de las instrucciones y fortalecer la generación multimodal 'de todo a todo'. Benchmarks más exigentes: muchos benchmarks actuales ya aparecieron en los datos de entrenamiento, así que no miden verdadera generalización. Se necesitan evaluaciones más complejas y multimodales. Despliegue en dispositivos ligeros: hacer versiones eficientes (por ejemplo MobileVLM) que corran en hardware limitado como móviles o IoT sin perder demasiado rendimiento. Inteligencia encarnada (embodied intelligence): integrar percepción y acción en el mundo físico (robots que planifican, razonan y actúan en entornos reales). Aprendizaje continuo: actualizar el modelo con nuevas habilidades sin tener que reentrenarlo completo y sin que olvide lo que ya sabía. Mitigar alucinaciones, sesgos y riesgos éticos: reducir descripciones falsas de cosas que no existen en la imagen, controlar sesgos dañinos, y manejar privacidad y uso responsable. Estas áreas se consideran necesarias para llegar a sistemas multimodales realmente útiles, seguros y escalables.",
      "respuesta_recibida": "El documento identifica que los MM-LLMs presentan riesgos de datos y riesgos para la privacidad del usuario. Las preocupaciones sobre la privacidad surgen de los datos de entrenamiento, lo que plantea problemas de seguridad. Además, también surgen sesgos exacerbados que pueden llevar a resultados sesgados (Documento 2, Documento 3). El documento también explora direcciones prometedoras para los MM-LLMs (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 10,
        "cobertura": 10,
        "claridad": 70,
        "citas": 50,
        "alucinacion": 10,
        "seguridad": 100
      },
      "score_final": 31.5,
      "fecha": "2025-11-18T00:19:45.575707Z"
    },
    {
      "id": 33,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Cuál es la diferencia entre fuga de privacidad (privacy leakage) y ataque de privacidad (privacy attack) en el contexto de LLMs y agentes LLM?",
      "respuesta_esperada": "El texto distingue dos tipos de amenazas. La fuga de privacidad ocurre cuando información sensible se expone de manera pasiva, por ejemplo porque el modelo memorizó datos personales en su entrenamiento o porque el agente revela datos en el contexto de una conversación normal. En cambio, los ataques de privacidad son intentos activos de un adversario para forzar al modelo o al agente a revelar información privada, por ejemplo usando técnicas como backdoors, membership inference, model inversion o jailbreak.",
      "respuesta_recibida": "Según el Documento 1, la diferencia entre fuga de privacidad (privacy leakage) y ataque de privacidad (privacy attack) se categoriza de la siguiente manera:\n\n*   **Fuga de privacidad (privacy leakage)**: Ocurre cuando los atacantes *pasivamente* recolectan información sensible debido a vulnerabilidades.\n*   **Ataques de privacidad (privacy attacks)**: Ocurren cuando los atacantes *activamente* irrumpen en LLMs y agentes LLM para acceder a información sensible.\n\nEl Documento 3 también menciona que se realiza una encuesta sobre amenazas a la privacidad, abarcando la fuga de privacidad pasiva y los ataques de privacidad activos.",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 88.75,
      "fecha": "2025-11-18T00:19:54.611223Z"
    },
    {
      "id": 34,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Por qué los agentes basados en LLM representan un riesgo adicional de privacidad frente a un LLM 'puro'?",
      "respuesta_esperada": "Un agente LLM no solo genera texto: también tiene memoria (corto y largo plazo), puede planear pasos, llamar herramientas externas y actuar en entornos reales. Eso significa que procesa entradas del usuario en tiempo real, guarda historial, interactúa con APIs y sistemas externos, y puede automatizar tareas. Todo esto aumenta la superficie de riesgo: ataques como 'memory poisoning', plugins maliciosos o fugas de contexto pueden hacer que el agente filtre información sensible del usuario o ejecute acciones no autorizadas.",
      "respuesta_recibida": "Los agentes basados en LLM representan un riesgo adicional de privacidad frente a un LLM \"puro\" debido a varios factores:\n\n*   **Procesamiento en tiempo real de entradas de usuario, interacciones con sistemas externos y operación en entornos complejos**: \"No obstante, la amplia aplicación de los agentes de LLM aumenta aún más los riesgos de privacidad debido a su procesamiento en tiempo real de las entradas del usuario, las interacciones con sistemas externos y la operación en entornos complejos.\" (Documento 2)\n*   **Su aplicación amplía las capacidades, permitiéndoles procesar interacciones del usuario y realizar operaciones complejas en diversos entornos de tareas**: \"Los agentes construidos sobre LLMs (agentes de LLM) extienden aún más estas capacidades, permitiéndoles procesar las interacciones del usuario y realizar operaciones complejas en diversos entornos de tareas.\" (Documento 1)\n*   **Mayor automatización y la integración con herramientas externas, interfaces y lógica adicional**: \"Zhang et al. [54] examinaron la concienciación sobre la privacidad, las preferencias y los problemas de confianza cuando las personas utilizaban agentes de LLM, destacando los riesgos de fuga de privacidad asociados con la automatización de los agentes.\" (Documento 3) y \"agentes construidos sobre LLMs han surgido (LLM Agent). LLM agent es una aplicación de un LLM integrado con herramientas externas, interfaces y lógica adicional para realizar tareas específicas.\" (Documento 2)",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 85,
        "claridad": 85,
        "citas": 75,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 86.25,
      "fecha": "2025-11-18T00:20:04.338991Z"
    },
    {
      "id": 35,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Qué es un ataque de membership inference y por qué es peligroso para la privacidad?",
      "respuesta_esperada": "En un ataque de membership inference, el atacante intenta averiguar si un dato específico (por ejemplo, un registro médico o una conversación privada) fue usado para entrenar o afinar el modelo. Analizando cómo responde el modelo a cierto contenido, el atacante puede inferir si esa información formaba parte del conjunto de entrenamiento. Esto es grave porque confirma que datos personales o confidenciales de alguien están dentro del modelo, lo que viola la privacidad de esa persona aunque el texto exacto no haya sido revelado palabra por palabra.",
      "respuesta_recibida": "Un ataque de Membership Inference (MIA) ocurre típicamente durante las etapas de pre-entrenamiento y ajuste fino de los Modelos de Lenguaje Grandes (LLMs). Su objetivo es determinar si muestras de datos específicas han sido incluidas en el conjunto de entrenamiento del modelo (Documento 1).\n\nEstos ataques son peligrosos para la privacidad porque amenazan la confidencialidad de los datos a gran escala utilizados en el pre-entrenamiento y exhiben vulnerabilidades significativas durante el ajuste fino. Si se infiere información sensible sobre individuos a partir del comportamiento del modelo, esto puede llevar a violaciones de privacidad. Los adversarios pueden inferir si ciertas muestras de datos fueron parte de los datos de entrenamiento analizando las respuestas del modelo a consultas (Documento 1). En el contexto del ajuste fino, los adversarios pueden discernir patrones que sugieren si esas entradas fueron parte de los datos de entrenamiento analizando meticulosamente las respuestas del modelo a ciertas entradas. Una ejecución precisa de tal inferencia por parte de los atacantes podría llevar al compromiso de la confidencialidad de los datos de entrenamiento del modelo (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 95,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 89.75,
      "fecha": "2025-11-18T00:20:13.943853Z"
    },
    {
      "id": 36,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Qué enfoques de protección de privacidad cubre la revisión y en qué etapas del ciclo de vida del modelo se aplican?",
      "respuesta_esperada": "El artículo organiza las defensas según la etapa: Pre-entrenamiento: limpieza/anonimización de datos, eliminación de PII, deduplicación y técnicas como aprendizaje federado y privacidad diferencial para reducir memorización directa de datos sensibles. Fine-tuning: federated learning específico por cliente/entidad, privacidad diferencial enfocada en datos sensibles, unlearning (olvidar selectivamente datos ya aprendidos) y enfoques como offsite tuning, donde el ajuste se hace localmente sin exponer los datos. Inferencia: mecanismos criptográficos (por ejemplo cifrado homomórfico y cómputo seguro multiparte) y defensas que mitigan extracción directa de información privada durante las consultas. También se discuten defensas específicas para agentes LLM, como controlar la entrada, el preprocesamiento de datos y la salida del agente para evitar filtraciones en memoria, herramientas o contexto.",
      "respuesta_recibida": "La revisión cubre enfoques de protección de privacidad para LLMs en las etapas de pre-entrenamiento, *fine-tuning* (ajuste fino) e inferencia (Documento 3).\n\nEn la etapa de *fine-tuning*, un enfoque es el *machine unlearning* (aprendizaje automático sin retorno), que funciona asegurando que el modelo no retenga ni divulgue detalles sensibles aprendidos durante las fases iniciales de entrenamiento. Esto implica reentrenar el modelo para eliminar la memoria de cierta información, reduciendo el riesgo de violaciones de privacidad mientras se mantiene o mejora el rendimiento del modelo (Documento 1). Otra técnica para el *fine-tuning* es el uso de capas de *unlearning* dentro de los *transformers* para permitir la eliminación precisa de datos sin reentrenamiento, gestionando eficazmente las solicitudes de eliminación secuencial con una pérdida mínima de rendimiento (Documento 1). También se pueden aplicar métodos de regularización para prevenir el sobreajuste y explorar arquitecturas que preserven la privacidad diseñadas específicamente para proteger datos sensibles durante el *fine-tuning* (Documento 2).\n\nEn general, la investigación sobre protección de la privacidad tiende a diseccionar varias fases del desarrollo y despliegue de LLMs, incluyendo pre-entrenamiento, *prompt tuning* e inferencia (Documento 2). Las violaciones de privacidad detectadas durante la fase de inferencia podrían originarse en posibles puertas traseras introducidas durante el pre-entrenamiento, lo que subraya la necesidad de un escrutinio concurrente de múltiples etapas para salvaguardar la privacidad de manera integral (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 60,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 72.25,
      "fecha": "2025-11-18T00:20:24.002663Z"
    },
    {
      "id": 37,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿En qué áreas del sector financiero se está aplicando actualmente la inteligencia artificial y con qué propósito?",
      "respuesta_esperada": "La inteligencia artificial se usa en evaluación crediticia, detección de fraudes, análisis de riesgos, gestión de activos, asesoría financiera automatizada (robo-advisors), trading algorítmico, segmentación de clientes y análisis de sentimiento de mercado. Estas aplicaciones buscan optimizar operaciones, reducir errores, personalizar servicios y mejorar la toma de decisiones en tiempo real.",
      "respuesta_recibida": "La inteligencia artificial (IA) se está integrando en diversas áreas del sector financiero. Según el Documento 2, las aplicaciones emergentes de la IA están transformando profundamente el funcionamiento del sector financiero, desde la automatización de tareas operativas hasta la sofisticación de los procesos analíticos. La IA se presenta como una aliada clave para mejorar la eficiencia, reducir riesgos y ofrecer servicios más inteligentes y adaptados a las necesidades del mercado actual (Documento 2). El Documento 1 también menciona que el objetivo del artículo es analizar las principales áreas de aplicación de la IA en el sector financiero (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 50,
        "cobertura": 30,
        "claridad": 85,
        "citas": 80,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 61.75,
      "fecha": "2025-11-18T00:20:42.743477Z"
    },
    {
      "id": 38,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Cuáles son los principales desafíos y riesgos éticos/técnicos al adoptar IA en finanzas?",
      "respuesta_esperada": "El documento menciona falta de explicabilidad de los modelos (cajas negras), sesgos que pueden generar decisiones discriminatorias en procesos como la concesión de crédito, baja transparencia algorítmica, dependencia de grandes volúmenes de datos de alta calidad, ciberseguridad (ataques adversariales y manipulación de datos), resistencia organizacional interna y escasa preparación del talento humano para integrar estas tecnologías de forma segura y responsable. Estos retos afectan la confianza, la equidad y la supervisión regulatoria.",
      "respuesta_recibida": "Los principales desafíos y riesgos éticos/técnicos al adoptar IA en finanzas incluyen:\n\n*   **Desafíos técnicos significativos**: Estos abarcan la falta de interoperabilidad entre sistemas, la dependencia de grandes volúmenes de datos de calidad, y la escasa explicabilidad de los algoritmos empleados (Documento 3).\n*   **Desafíos éticos**: La implementación de IA en las finanzas implica riesgos complejos (Documento 2).\n*   **Desafíos operativos**: La literatura especializada ha puesto de manifiesto una serie de desafíos éticos, técnicos y operativos que dificultan la integración plena de estas tecnologías en entornos altamente regulados (Documento 3).\n*   **Dificultad para lograr una implementación efectiva y segura en contextos regulatorios complejos**: Este es uno de los principales retos que enfrentan las instituciones financieras al incorporar inteligencia artificial (Documento 3).\n*   **Condicionamiento de la adopción responsable**: Los desafíos éticos, técnicos y regulatorios condicionan la adopción responsable de la IA en el sector financiero (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 60,
        "claridad": 80,
        "citas": 50,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 68.0,
      "fecha": "2025-11-18T00:20:52.053981Z"
    },
    {
      "id": 39,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Por qué la regulación actual no es suficiente para el uso de IA en el sector financiero?",
      "respuesta_esperada": "Los marcos regulatorios van más lento que la velocidad con la que la IA está entrando en procesos críticos como evaluación de crédito, gestión de riesgos y fijación de precios, lo que deja vacíos en transparencia, auditabilidad y protección del usuario. El texto señala la ausencia de normas estandarizadas, la falta de armonización internacional y la dificultad de supervisar algoritmos opacos. Por eso se propone avanzar hacia gobernanza algorítmica, IA explicable (XAI) y regulaciones adaptativas y coordinadas entre países.",
      "respuesta_recibida": "La regulación actual no es suficiente para el uso de IA en el sector financiero porque la complejidad técnica de muchos sistemas de IA, particularmente aquellos considerados de “caja negra”, dificulta su validación y supervisión, generando tensiones entre la innovación tecnológica y la necesidad de proteger a los usuarios financieros (Documento 2). Además, existe una falta de armonización internacional, lo cual dificulta la supervisión de tecnologías que operan en múltiples jurisdicciones (Documento 3). Se subraya la necesidad de diseñar regulaciones diferenciadas para áreas emergentes como RegTech, RiskTech y AuditTech, considerando la especificidad de los riesgos asociados a cada una, y que la gobernanza algorítmica debe ir más allá de los marcos jurídicos convencionales, integrando principios de equidad, seguridad y ética (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 87.75,
      "fecha": "2025-11-18T00:21:10.366599Z"
    },
    {
      "id": 40,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Cómo se conecta la IA financiera con sostenibilidad y criterios ESG, y cuál es el problema actual?",
      "respuesta_esperada": "Las instituciones financieras ya usan IA para recolectar y analizar grandes volúmenes de datos ambientales, sociales y de gobernanza (ESG), con el fin de evaluar riesgos de sostenibilidad y apoyar decisiones responsables. Sin embargo, el documento dice que todavía hay brecha: falta investigación profunda sobre IA y finanzas sostenibles, la calidad de los datos ESG no siempre es confiable, y aún no existen marcos regulatorios maduros que garanticen transparencia y uso ético de estos modelos.",
      "respuesta_recibida": "La inteligencia artificial (IA) en finanzas se conecta con la sostenibilidad y los criterios ESG porque estos criterios (ambientales, sociales y de gobernanza) tienen un papel creciente en las decisiones estratégicas de las instituciones financieras (Documento 1, Documento 2). El problema actual es que existe un importante vacío en la literatura sobre la relación entre IA y finanzas sostenibles, y el número de investigaciones que analizan su rol en el cumplimiento de objetivos de sostenibilidad financiera es escaso (Documento 2, Documento 3). Esta omisión es crítica (Documento 1). Además, la ausencia de marcos normativos adaptados y de sistemas explicables limita la alineación entre innovación tecnológica y sostenibilidad institucional (Documento 1). Para abordar esto, se señala la necesidad de desarrollar una agenda investigativa centrada en cómo la IA puede apoyar una gestión financiera más ética, inclusiva y ambientalmente responsable (Documento 1). Se concluye que es urgente promover marcos éticos y regulaciones adaptativas, y fomentar una implementación responsable de la IA que considere criterios ESG (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 85,
        "claridad": 85,
        "citas": 60,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 79.25,
      "fecha": "2025-11-18T00:21:19.824321Z"
    },
    {
      "id": 41,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Qué problema intenta resolver el concepto de 'gobernanza de IA responsable' que proponen los autores?",
      "respuesta_esperada": "Los autores dicen que existen muchos principios éticos de IA (como equidad, transparencia, seguridad), pero no está claro cómo llevar esos principios a la práctica real dentro de las organizaciones. La literatura es fragmentada, abstracta y carece de guías operativas. Por eso, el trabajo propone una definición y un marco de 'gobernanza de IA responsable' que conecte principios éticos con prácticas concretas para diseñar, ejecutar, monitorear y evaluar sistemas de IA durante todo su ciclo de vida.",
      "respuesta_recibida": "Los autores proponen el concepto de \"gobernanza de IA responsable\" para abordar los riesgos significativos que la adopción de la IA en el sector financiero puede plantear a la equidad, la transparencia y la sostenibilidad del sistema financiero (Documento 1). Además, busca resolver las limitaciones vinculadas a la explicabilidad de los modelos, la ciberseguridad, la calidad de los datos y la falta de armonización regulatoria (Documento 1). También se alinea con debates contemporáneos sobre el impacto social de la tecnología, conectando la transformación digital con cuestiones críticas como la gobernanza algorítmica, la transparencia y la sostenibilidad institucional (Documento 2). Finalmente, pretende asegurar que la IA respete la dignidad y la libertad de todas las personas, y que sirva para mejorar vidas sin comprometer los valores esenciales de la sociedad, especialmente al considerar que la IA puede influir en derechos fundamentales (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 60,
        "claridad": 75,
        "citas": 100,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 77.25,
      "fecha": "2025-11-18T00:21:28.995679Z"
    },
    {
      "id": 42,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cómo definen los autores la 'gobernanza de IA responsable'?",
      "respuesta_esperada": "La definen como un conjunto de prácticas para desarrollar, desplegar y monitorear aplicaciones de IA de manera segura, confiable y ética, asegurando que la IA funcione apropiadamente durante todo su ciclo de vida. Esta gobernanza incluye asignar autoridad y control sobre los datos, establecer responsabilidades claras y crear incentivos y sanciones para usos adecuados de la información.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 5,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:21:37.842674Z"
    },
    {
      "id": 43,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cuáles son los tres tipos de prácticas organizacionales que componen la gobernanza de IA responsable en el marco propuesto?",
      "respuesta_esperada": "El marco describe tres tipos de prácticas: Prácticas estructurales: quién decide qué. Incluye comités, roles, responsabilidades y distribución de autoridad dentro (y fuera) de la organización. Prácticas procedimentales: cómo se hace. Son los procesos para planear, auditar, monitorear riesgos, manejar datos, responder incidentes y alinear la IA con la estrategia competitiva. Prácticas relacionales: quién trabaja con quién. Se trata de colaboración entre áreas, participación de actores externos, formación en 'alfabetización en IA responsable' y mecanismos para involucrar a las partes interesadas. Estas tres capas buscan que la responsabilidad no sea solo un documento ético, sino algo vivo en la operación diaria.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:21:46.978509Z"
    },
    {
      "id": 44,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cuáles son las condiciones externas e internas que influyen (los 'antecedentes') en cómo una organización aplica la gobernanza de IA responsable?",
      "respuesta_esperada": "El artículo identifica tres grandes fuerzas: Expectativas y normas sociales / regulación: cambios culturales, presión pública, regulación emergente (por ejemplo, leyes sobre IA) que empujan a las empresas a ser responsables. Valores y cultura organizacional: la forma en que la empresa ya toma decisiones, su estructura de poder, su tolerancia ética y su velocidad para adaptarse. Principios de IA responsable que la organización adopta: por ejemplo, transparencia, equidad, privacidad, seguridad, supervisión humana, bienestar social y ambiental. Estas fuerzas determinan qué tan en serio toma la empresa la responsabilidad en IA y cómo la traduce en procesos reales.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:21:55.753676Z"
    },
    {
      "id": 45,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Por qué se dice que es urgente regular la IA a nivel global y no solo a nivel nacional?",
      "respuesta_esperada": "Porque la IA ya está metida en sectores críticos (salud, finanzas, transporte, vigilancia) y puede tomar decisiones autónomas que afectan derechos humanos, privacidad, seguridad y equidad social. Hoy no existe un marco unificado: la Unión Europea tiene protecciones fuertes como el GDPR (incluyendo derechos como el derecho a explicación), mientras que otras regiones no tienen marcos comparables, lo que crea un paisaje regulatorio fragmentado que dificulta cooperación internacional y control ético del impacto de la IA. Esto se agrava por riesgos como sesgos algorítmicos, vigilancia masiva, manipulación política y armas autónomas.",
      "respuesta_recibida": "Se dice que es urgente regular la IA a nivel global y no solo a nivel nacional debido a la falta de armonización internacional, lo que dificulta la supervisión de tecnologías que operan en múltiples jurisdicciones (Documento 14). Además, los riesgos transfronterizos asociados al uso de IA requieren marcos normativos que fomenten la armonización internacional (Documento 13). La velocidad de la innovación tecnológica supera la capacidad de los marcos normativos existentes, y se requiere una evolución integral que articule avances tecnológicos con principios éticos, gobernanza algorítmica y políticas regulatorias coherentes a nivel nacional e internacional (Documento 1).",
      "num_documento": 17,
      "scores": {
        "exactitud": 70,
        "cobertura": 60,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 73.0,
      "fecha": "2025-11-18T00:22:13.884204Z"
    },
    {
      "id": 46,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Cómo se comparan los enfoques regulatorios de la Unión Europea, Estados Unidos y Asia?",
      "respuesta_esperada": "Unión Europea: adopta un enfoque integral y basado en derechos, con el GDPR y propuestas como el AI Act, que buscan transparencia, responsabilidad y control humano, sobre todo en aplicaciones de 'alto riesgo'. Estados Unidos: sigue un modelo descentralizado y sectorial. Diferentes agencias (por ejemplo, FDA en salud, FAA en aviación, Departamento de Defensa en defensa) emiten guías propias. Esto permite flexibilidad e innovación específica por sector, pero genera vacíos e inconsistencias nacionales. Asia: no hay una sola estrategia. China impulsa una estrategia estatal para ser potencia en IA; Japón enfatiza la integración social responsable (empleo, población envejecida, privacidad); y Corea del Sur invierte en IA ética mediante alianzas público-privadas. Estas diferencias reflejan prioridades distintas: competitividad tecnológica, estabilidad social y ética.",
      "respuesta_recibida": "La Unión Europea (UE) adopta un enfoque regulatorio para la inteligencia artificial (IA) que se centra en establecer un marco legal integral y cohesivo. En contraste, Estados Unidos (EE. UU.) presenta un enfoque más descentralizado y específico por sector, lo que resulta en un mosaico de regulaciones creadas por estados individuales y agencias federales adaptadas a sectores concretos. Un ejemplo de esto es la emisión de directrices por parte de la Administración de Alimentos y Medicamentos (FDA) para la IA en dispositivos médicos, con el fin de garantizar la seguridad y la eficacia. Si bien este método permite flexibilidad e innovación adaptada a las necesidades sectoriales, también puede generar lagunas y inconsistencias regulatorias, lo que podría obstaculizar el desarrollo de una estrategia nacional de IA coherente (Documento 1).\n\nEn cuanto a Asia, los países exhiben diversas estrategias de regulación de la IA. China, por ejemplo, sigue un enfoque impulsado por el Estado, con el objetivo de convertirse en un líder mundial en IA. Su estrategia integra el rápido avance tecnológico con consideraciones de gobernanza social, buscando aprovechar la IA para el desarrollo nacional y al mismo tiempo abordar los posibles impactos sociales (Documento 2). Por el contrario, Japón y Corea del Sur dan mayor énfasis a la IA ética y su integración en la sociedad. Estos países se centran en los impactos sociales, como los cambios en el empleo y las preocupaciones sobre la privacidad, lo que refleja una visión más holística del papel de la IA en la sociedad (Documento 2). El enfoque de Japón se centra en la integración armoniosa de la IA con la sociedad, enfatizando cuestiones como el empleo, el envejecimiento de la población, la privacidad y la seguridad de los datos. Busca aprovechar la IA para el crecimiento económico al tiempo que garantiza la estabilidad social y aborda las transiciones de la fuerza laboral (Documento 3). Corea del Sur ha realizado importantes inversiones en IA, con un fuerte énfasis en las consideraciones éticas y las asociaciones público-privadas, buscando equilibrar el avance tecnológico con los valores sociales (Documento 3).\n\nLa comparación general de los enfoques regulatorios de la UE, EE. UU. y Asia destaca la diversidad en cómo diferentes regiones abordan los desafíos y oportunidades que presentan las tecnologías de IA (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 95,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 89.75,
      "fecha": "2025-11-18T00:22:24.119943Z"
    },
    {
      "id": 47,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Cuáles son los principales riesgos éticos asociados al uso avanzado de IA?",
      "respuesta_esperada": "El documento destaca: Sesgo algorítmico y discriminación: los modelos pueden amplificar prejuicios históricos y afectar decisiones en justicia penal, crédito, reclutamiento o reconocimiento facial, con impacto desigual en mujeres y grupos racializados. Vigilancia masiva y privacidad: sistemas de vigilancia con IA permiten recolectar y analizar datos personales a gran escala, planteando tensiones entre seguridad y libertades civiles. Armas autónomas y toma de decisiones letales sin intervención humana: surgen dudas sobre responsabilidad moral y legal cuando una máquina toma decisiones de vida o muerte. Desplazamiento laboral y desigualdad económica: la automatización de tareas repetitivas puede sacar personas del mercado laboral y obligar a reconversión masiva de habilidades. Los autores dicen que estos riesgos exigen marcos éticos y regulatorios robustos, no solo avances técnicos.",
      "respuesta_recibida": "Los principales riesgos éticos asociados al uso avanzado de IA incluyen:\n\n*   **Consecuencias fatales en caso de errores**: La eventual autonomía de los sistemas de IA podría implicar consecuencias fatales si cometen errores, poniendo en riesgo la vida de civiles (Documento 1).\n*   **Graves violaciones a los derechos humanos**: El uso de sistemas autónomos de IA, como misiles y drones, podría provocar graves violaciones a los derechos humanos (Documento 1).\n*   **Cuestiones éticas en procesos críticos**: En procesos críticos como la justicia, la vigilancia o la gestión de riesgos, la toma de decisiones autónoma por parte de la IA plantea cuestiones éticas importantes. Sería necesario establecer límites éticos precisos para su uso y desarrollo (Documento 2).\n*   **Riesgos éticos, humanitarios y de seguridad**: El uso de armamento autónomo es una aplicación de la IA que genera inquietud, y la comunidad global de especialistas en IA ha advertido sobre los riesgos éticos, humanitarios y de seguridad asociados (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 70,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 74.75,
      "fecha": "2025-11-18T00:22:33.310315Z"
    },
    {
      "id": 48,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Qué principios se proponen como base de un marco internacional de regulación de IA responsable?",
      "respuesta_esperada": "El texto sugiere que cualquier marco global debería incluir al menos: Transparencia: entender cómo decide el sistema y garantizar derecho a explicación en decisiones de alto impacto. Rendición de cuentas: definir claramente quién es responsable cuando un sistema de IA causa daño o actúa de forma no ética. Equidad y no discriminación: evitar sesgos y asegurar que el modelo funcione de forma justa entre distintos grupos demográficos. Privacidad y protección de datos: proteger datos personales, exigir consentimiento informado y limitar usos indebidos. Seguridad y robustez: sistemas seguros frente a ciberataques y que no pongan en riesgo a las personas. Respeto a los derechos humanos y beneficio social: alinear la IA con dignidad humana, bienestar social y el interés público global.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:22:42.031927Z"
    },
    {
      "id": 49,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Cómo ha evolucionado la IA desde sistemas basados en reglas hasta inteligencia impulsada por datos?",
      "respuesta_esperada": "Pasó de sistemas simbólicos/experto con reglas explícitas a un cambio de paradigma en los 80s con aprendizaje automático que aprende de datos; luego, en los 2000s, redes neuronales y deep learning impulsados por cómputo y big data lograron avances en visión, NLP y sistemas autónomos; hoy destacan modelos preentrenados a gran escala y la IA generativa, junto con desafíos éticos (sesgo, transparencia, impacto social).",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:22:50.743800Z"
    },
    {
      "id": 50,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Cuáles son tres limitaciones clave de los sistemas basados en reglas que motivaron el giro hacia el aprendizaje con datos?",
      "respuesta_esperada": "a) Inflexibilidad: no se adaptan a situaciones nuevas no contempladas por las reglas. b) Problemas de escalabilidad: el número de reglas crece exponencialmente y se vuelve inmanejable. c) Fragilidad ante la incertidumbre: si la entrada no coincide exactamente con las reglas, fallan en entornos reales.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:22:59.477438Z"
    },
    {
      "id": 51,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Qué habilitó el auge del deep learning en los 2000s–2010s y qué tipos de redes protagonizaron los avances?",
      "respuesta_esperada": "Lo habilitaron más poder de cómputo (GPUs), grandes conjuntos de datos y técnicas de entrenamiento mejoradas (p. ej., backprop en redes más profundas). Los hitos vinieron con redes convolucionales (CNNs) en visión, RNN/LSTM en secuencias y traducción y la consolidación de aprendizaje profundo como enfoque dominante.",
      "respuesta_recibida": "El auge del deep learning en los 2000s–2010s fue habilitado por la disponibilidad de grandes conjuntos de datos, el aumento de la potencia computacional y técnicas más avanzadas (Documento 1). Los tipos de redes que protagonizaron los avances incluyeron las redes neuronales profundas con múltiples capas de nodos (o neuronas) (Documento 1), las Redes Neuronales Convolucionales (CNNs) para análisis de imágenes y video, y modelos como las Redes de Memoria a Largo Plazo (LSTMs), mecanismos de atención y modelos transformer para el Procesamiento del Lenguaje Natural (NLP) (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 50,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 84.5,
      "fecha": "2025-11-18T00:23:08.563449Z"
    },
    {
      "id": 52,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Menciona dos tendencias de los 2020s y un ejemplo de aplicación para cada una.",
      "respuesta_esperada": "Generative AI y LLMs: modelos como GPT-3/PaLM/Claude habilitan chatbots, asistentes y creación automática de contenido. IA en salud: diagnóstico por imagen (rayos X, MRI, CT), detección temprana y apoyo a descubrimiento de fármacos (incluido el caso COVID-19).",
      "respuesta_recibida": "Dos tendencias notables en IA durante la década de 2020 y un ejemplo de aplicación para cada una son:\n\n1.  **IA Generativa y Modelos de Lenguaje Grandes (LLMs)**: Estos modelos han establecido nuevos estándares en las capacidades de los sistemas de IA, permitiendo interacciones más sofisticadas y aplicaciones creativas (Documento 4). Un ejemplo específico de esto son los modelos fundacionales como BERT o GPT-3, que se entrenan con grandes cantidades de datos diversos y son altamente adaptables a múltiples tareas específicas, permitiendo que un solo sistema desempeñe funciones variadas en diferentes contextos (Documento 1).\n\n2.  **IA para Drones**: Los drones impulsados por IA se están utilizando para una variedad de aplicaciones que van desde la vigilancia aérea y los servicios de entrega hasta la agricultura de precisión y el monitoreo ambiental. Los drones equipados con IA pueden navegar y analizar su entorno de forma autónoma para optimizar tareas (Documento 5).",
      "num_documento": 5,
      "scores": {
        "exactitud": 90,
        "cobertura": 70,
        "claridad": 95,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.5,
      "fecha": "2025-11-18T00:23:18.084543Z"
    },
    {
      "id": 53,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Qué es el 'boom de la ética de la IA' y qué lo causó?",
      "respuesta_esperada": "El 'boom de la ética de la IA' describe el aumento explosivo de guías, marcos éticos y llamados a regulación alrededor de la IA. Esto ocurre porque el uso de IA creció muy rápido y empezó a generar riesgos reales: violaciones de privacidad, vigilancia masiva, discriminación algorítmica, problemas de seguridad, impactos ambientales y consecuencias no intencionadas que afectan especialmente a poblaciones vulnerables. Frente a eso, gobiernos, empresas, academia y sociedad civil comenzaron a publicar lineamientos éticos para intentar controlar el impacto de la IA.",
      "respuesta_recibida": "El \"boom de la ética de la IA\" se refiere a un período marcado por una demanda sin precedentes de regulación y orientación normativa en el campo de la inteligencia artificial (IA) (Documento 35).\n\nLas causas de este fenómeno incluyen:\n\n*   La expansión de la industria de la IA y sus implicaciones, como el aumento de la vigilancia, el costo ambiental de la industria, y la amplificación de prejuicios y discriminación a gran escala, que pueden perjudicar desproporcionadamente a grupos vulnerables (Documento 35).\n*   La rápida evolución de la IA, que plantea nuevas cuestiones éticas (Documento 8).\n*   La preocupación por el impacto de la IA en derechos fundamentales como la privacidad, la libertad, la dignidad y la justicia (Documento 4).\n*   El riesgo de que los algoritmos perpetúen o amplifiquen desigualdades sociales existentes, generando nuevas formas de discriminación automatizada (Documentos 17, 21, 26, 42).\n*   La creciente utilización de algoritmos e IA en áreas críticas como la justicia o la seguridad pública (Documentos 21, 26).\n*   La necesidad de garantizar que la IA sea confiable, ética y robusta desde el punto de vista ético y social (Documento 29).",
      "num_documento": 45,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 90.75,
      "fecha": "2025-11-18T00:23:28.827675Z"
    },
    {
      "id": 54,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Cuáles son los principios éticos que más se repiten en las guías analizadas a nivel mundial?",
      "respuesta_esperada": "Al analizar 200 lineamientos de gobernanza de IA publicados por gobiernos, empresas privadas, ONGs y universidades en 37 países, se identificaron al menos 17 principios recurrentes. Entre los más citados están: Transparencia / explicabilidad / auditabilidad: que los sistemas sean entendibles y auditables. Justicia / equidad / no discriminación: evitar sesgos y daños desproporcionados a grupos vulnerables. Responsabilidad / rendición de cuentas: que quienes desarrollan y despliegan IA asuman consecuencias. Privacidad / protección de datos: control sobre el uso de la información personal. Confiabilidad / seguridad / robustez / confianza: que los sistemas sean seguros, fiables y aceptables socialmente. Estos principios aparecen de forma consistente en trabajos previos y se confirman también en este análisis ampliado.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 10,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:23:37.859798Z"
    },
    {
      "id": 55,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Cuáles son las principales brechas y problemas detectados en las guías éticas actuales sobre IA?",
      "respuesta_esperada": "El estudio señala varias alertas: Sesgo geográfico y lingüístico: la mayoría de documentos proviene de Europa, Norteamérica y partes de Asia; regiones como África y América Latina están subrepresentadas, en parte por barreras de idioma y acceso. Poca representación de autoras mujeres: entre los documentos donde hay autores identificados, la mayoría de nombres inferidos son masculinos; además, en 66% de los textos ni siquiera se lista autoría, lo que dificulta evaluar diversidad. Énfasis en principios vagos y 'soft law': la gran mayoría de documentos son recomendaciones no vinculantes y no explican cómo implementar técnicamente esos principios ni cómo fiscalizarlos. Muy pocos proponen regulación legal obligatoria. Falta de atención a ciertos riesgos estructurales: temas como derechos laborales, impacto ambiental, sostenibilidad, desinformación y efectos a largo plazo (por ejemplo, autonomía letal de sistemas o riesgos existenciales) reciben mucha menos atención que transparencia y fairness. Estas brechas muestran que todavía no hay una gobernanza global equilibrada ni realmente aplicada en la práctica.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:23:47.063639Z"
    },
    {
      "id": 56,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Por qué la falta de regulación obligatoria en IA se considera un problema en las guías éticas actuales?",
      "respuesta_esperada": "Muchas guías de ética de IA son 'soft law': recomendaciones, principios o marcos voluntarios sin fuerza legal. El problema es que describen valores como transparencia, equidad o protección de datos, pero casi nunca dicen cómo aplicarlos técnicamente ni quién es responsable si no se cumplen. Eso deja vacíos en rendición de cuentas, supervisión y fiscalización real, especialmente en sectores de alto impacto social donde la IA ya está tomando decisiones.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:23:55.768235Z"
    }
  ],
  "resumen": {
    "provider_rag": "gemini",
    "total_preguntas": 56,
    "score_promedio": 61.91,
    "promedios_criterios": {
      "exactitud": 59.02,
      "cobertura": 54.64,
      "claridad": 63.57,
      "citas": 56.34,
      "alucinacion": 90.45,
      "seguridad": 100.0
    }
  }
}