{
  "provider_rag": "gemini",
  "fecha_evaluacion": "2025-11-18T00:52:02.076519Z",
  "formula": "Score = 0.35*Exactitud + 0.20*Cobertura + 0.15*Claridad + 0.20*Citas + 0.05*Alucinación + 0.05*Seguridad",
  "resultados": [
    {
      "id": 1,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Cuáles son los tres ejes principales del estudio de los agentes según Wooldridge y Jennings?",
      "respuesta_esperada": "Los autores dividen el estudio de los agentes en tres áreas principales: Teorías de agentes (Agent theories): tratan de definir qué es un agente y cómo representar formalmente sus propiedades. Arquitecturas de agentes (Agent architectures): se enfocan en cómo construir agentes que cumplan con las propiedades esperadas, tanto en software como en hardware. Lenguajes de agentes (Agent languages): se centran en cómo programar agentes y qué primitivas o estructuras son adecuadas para su implementación.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:39:05.067052Z"
    },
    {
      "id": 2,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué diferencia existe entre un agente deliberativo y uno reactivo?",
      "respuesta_esperada": "Un agente deliberativo se basa en modelos simbólicos del mundo y toma decisiones mediante razonamiento lógico o planificación (por ejemplo, STRIPS o IRMA). Un agente reactivo no utiliza representaciones simbólicas ni razonamiento complejo; responde directamente a los estímulos del entorno. Ejemplos de este tipo son los robots basados en la arquitectura de subsunción de Brooks.",
      "respuesta_recibida": "Un agente reactivo se define como aquel que no incluye ningún tipo de modelo simbólico central del mundo y no utiliza razonamiento simbólico complejo (Documento 25). Por el contrario, los agentes deliberativos, aunque no se definen explícitamente, se mencionan en contraposición a los reactivos, sugiriendo que sí emplean un modelo simbólico central del mundo y razonamiento complejo (Documento 14, Documento 25).",
      "num_documento": 25,
      "scores": {
        "exactitud": 85,
        "cobertura": 70,
        "claridad": 80,
        "citas": 60,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 77.5,
      "fecha": "2025-11-18T00:39:20.027312Z"
    },
    {
      "id": 3,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué plantea la teoría de la intención de Cohen y Levesque?",
      "respuesta_esperada": "Cohen y Levesque proponen una lógica formal para representar la intención de los agentes como un equilibrio racional entre creencias y metas. Según ellos, los agentes deben: 1. Actuar conforme a sus intenciones. 2. Adoptar solo intenciones factibles. 3. Mantener o abandonar intenciones dependiendo del cambio de creencias. Esta teoría fue clave para modelar agentes racionales y la cooperación en sistemas multiagente.",
      "respuesta_recibida": "La teoría de la intención de Cohen y Levesque adopta un enfoque de dos niveles para formalizar una teoría de la intención. Primero, construyen una lógica de la agencia racional, clasificando cuidadosamente las relaciones entre los operadores modales básicos. Sobre esta base, introducen constructos derivados que forman una \"teoría parcial de la acción racional\", donde la intención es uno de esos constructos. Sintácticamente, su lógica es una lógica multimodal, cuantificada y de muchos tipos con igualdad, que contiene cuatro modalidades primarias: (BEL x ϕ) Agente x cree que ϕ, (GOAL x ϕ) Agente x tiene el objetivo de ϕ, (HAPPENS α) La acción α ocurrirá a continuación, y (DONE α) La acción α acaba de ocurrir. [28]\n\nSe menciona que los agentes no necesitan tener la intención de todos los efectos secundarios esperados de sus intenciones. [2]\n\nEn el trabajo de Cohen y Levesque, se utilizaron dos actitudes básicas: creencias y objetivos. Actitudes adicionales, como la intención, se definieron en términos de estas. [3]",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 60,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 76.0,
      "fecha": "2025-11-18T00:39:33.511550Z"
    },
    {
      "id": 4,
      "archivo": "Agent Theories, Architectures, and Languages: A Survey",
      "pregunta": "¿Qué es la programación orientada a agentes (Agent-Oriented Programming, AOP) según Shoham?",
      "respuesta_esperada": "Shoham propone la AOP como un nuevo paradigma de programación donde los agentes se programan directamente usando nociones mentales como creencias, compromisos y habilidades. Su sistema AGENT0 permite especificar agentes con: Capacidades (acciones posibles), Creencias y compromisos iniciales, Reglas de compromiso que determinan su comportamiento ante mensajes o cambios de estado.",
      "respuesta_recibida": "Según Shoham, la programación orientada a agentes (AOP) es un \"nuevo paradigma de programación, basado en una visión societal de la computación\" ([Documento 1]). La idea clave de este paradigma es programar directamente a los agentes en términos de las nociones mentalistas e intencionales que los teóricos de agentes han desarrollado para representar las propiedades de los agentes ([Documento 1]). La motivación detrás de esta propuesta es que, al igual que los humanos utilizan la \"postura intencional\" como mecanismo de abstracción para representar las propiedades de sistemas complejos, podría ser útil utilizar la postura intencional para programar máquinas ([Documento 2]).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 70,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 84.0,
      "fecha": "2025-11-18T00:39:46.023696Z"
    },
    {
      "id": 5,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Cuáles son los principales riesgos éticos asociados al uso de la inteligencia artificial según el autor?",
      "respuesta_esperada": "El texto identifica varios riesgos éticos clave: la falta de transparencia (la 'caja negra' algorítmica), la amplificación de sesgos preexistentes, la reducción de la supervisión humana en decisiones críticas y la vulneración de la privacidad a través de la vigilancia masiva. Estos problemas pueden provocar discriminación y erosionar derechos fundamentales como la intimidad y la libertad de expresión.",
      "respuesta_recibida": "Los principales riesgos éticos asociados al uso de la inteligencia artificial según los autores incluyen:\n\n*   **Sesgo y discriminación**: Las decisiones y predicciones de la IA dependen de los datos con los que ha sido entrenada. Si estos datos contienen sesgos, el algoritmo los reflejará, lo que puede traducirse en consecuencias desiguales y perjudiciales para ciertos grupos, perpetuando prejuicios sociales (Documento 1). Esto es especialmente relevante en ámbitos como la justicia, donde se espera imparcialidad e igualdad ante la ley (Documento 1).\n*   **Uso malintencionado y ciberdelincuencia**: La IA puede ser utilizada con fines delictivos, como ciberdelincuencia, suplantación de identidades y estafas (Documento 2).\n*   **Desigualdad en el acceso**: La brecha digital puede generar una desigualdad en el acceso a la IA, incrementando la desigualdad social y dejando a algunas poblaciones en desventaja (Documento 2).\n*   **Impacto social y laboral**: Existe el temor a la deshumanización de ciertos trabajos y a la pérdida de empleo, especialmente entre los sectores más vulnerables o aquellos en puestos precarios (Documento 2).\n*   **Falta de transparencia y marco regulador inadecuado**: Los algoritmos que operan con sesgos, carecen de transparencia o se implementan sin un marco regulador adecuado presentan riesgos importantes (Documento 3).\n\nLa necesidad de establecer límites éticos precisos para el uso y desarrollo de la IA, especialmente en procesos críticos como la justicia, la vigilancia o la gestión de riesgos, es crucial (Documento 1). La seguridad en el uso de la IA debe estar alineada con principios de justicia, equidad y respeto por los derechos fundamentales para evitar consecuencias negativas para la sociedad (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 80,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.0,
      "fecha": "2025-11-18T00:39:59.843790Z"
    },
    {
      "id": 6,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Qué relación existe entre la inteligencia artificial y la vulneración de los derechos humanos?",
      "respuesta_esperada": "El autor explica que los algoritmos, al operar sobre datos cargados de prejuicios sociales, pueden perpetuar desigualdades y discriminaciones, afectando derechos como la igualdad ante la ley, la privacidad y la libertad individual. En el ámbito judicial y de seguridad, la IA puede transformarse en una herramienta de control en lugar de garantizar justicia y equidad.",
      "respuesta_recibida": "La Inteligencia Artificial (IA) puede afectar los derechos fundamentales, haciendo hincapié en la vulneración de derechos como la intimidad y la libertad de expresión (Documento 1). La IA, aunque prometedora, no está exenta de riesgos, especialmente cuando su desarrollo y aplicación se enfrentan a marcos regulatorios aún en construcción (Documento 2). La implementación de la IA sin una regulación adecuada puede profundizar las desigualdades sociales y vulnerar principios democráticos esenciales, ya que los sistemas automatizados pueden reproducir sesgos preexistentes, afectando de manera desigual a determinados colectivos (Documento 3). La creciente utilización de algoritmos e IA en áreas críticas introduce un riesgo inherente de discriminación algorítmica, perpetuando y amplificando desigualdades sociales existentes, e incluso generando nuevas formas de discriminación automatizada (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 70,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 78.25,
      "fecha": "2025-11-18T00:40:13.046964Z"
    },
    {
      "id": 7,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Por qué se considera urgente establecer una regulación sobre la inteligencia artificial?",
      "respuesta_esperada": "Porque el avance tecnológico supera la velocidad de la legislación actual. Sin marcos regulatorios claros, los algoritmos pueden vulnerar derechos fundamentales sin supervisión ni rendición de cuentas. La Agencia de los Derechos Fundamentales de la Unión Europea advierte que, sin regulación, los riesgos éticos y legales asociados con la IA podrían desbordar la capacidad de los sistemas de protección existentes.",
      "respuesta_recibida": "Se considera urgente establecer una regulación sobre la inteligencia artificial debido a que los sistemas de vigilancia inteligentes almacenan y procesan datos sensibles, lo que plantea interrogantes sobre el acceso a dicha información y los propósitos y criterios bajo los cuales se utiliza. Esto reaviva el debate sobre el derecho a la privacidad en la era digital, y la falta de transparencia en la recolección y análisis de datos podría derivar en serias vulneraciones de derechos. Sin una regulación clara y efectiva, la vigilancia masiva podría dar lugar a nuevas formas de control social que operen en la sombra, escapando al control ciudadano. (Documento 2)\n\nAdemás, se enfatiza la importancia de establecer mecanismos de verificación y auditación para los modelos de IA de alto riesgo, promover la formación en esta área y garantizar la transparencia de los algoritmos para evitar sesgos discriminatorios. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 50,
        "claridad": 85,
        "citas": 80,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 73.0,
      "fecha": "2025-11-18T00:40:25.544535Z"
    },
    {
      "id": 8,
      "archivo": "Algorithm, Ethics, and Risks of Artificial Intelligence: Implications for Human Rights",
      "pregunta": "¿Cómo puede el uso de la inteligencia artificial en ámbitos sensibles afectar la justicia y los derechos fundamentales?",
      "respuesta_esperada": "El uso de la inteligencia artificial en áreas como la seguridad o la justicia puede generar sesgos, decisiones injustas y pérdida de transparencia, afectando principios como la presunción de inocencia, la equidad y la privacidad. Si no se regula adecuadamente, la IA puede transformar procesos diseñados para proteger a las personas en sistemas de control que vulneren los derechos humanos.",
      "respuesta_recibida": "El uso de la inteligencia artificial (IA) en ámbitos sensibles como la Administración de Justicia puede afectar la justicia y los derechos fundamentales de diversas maneras. La IA, a través de tecnologías como la vigilancia masiva y la predicción del delito, puede vulnerar derechos como la intimidad y la libertad de expresión (Documento 1).\n\nLa IA es eficaz en el procesamiento de extensos volúmenes de información, lo que la hace idónea para objetivos en el tratamiento de la justicia y seguridad pública. Sin embargo, su uso masivo de datos puede conducir a una pérdida significativa de la privacidad personal, ya que recopila datos para sus propias predicciones y optimiza su actuación basándose en información de las personas. Esto plantea un dilema ético relacionado con el sacrificio de la privacidad de los ciudadanos en aras de una supuesta mejora de la seguridad (Documento 2).\n\nLa IA puede penetrar en los procesos humanos y tiene la capacidad de recoger y analizar información privada sin el consentimiento de los ciudadanos, lo que vulnera el derecho a la privacidad (Documento 3). Además, al identificar patrones basados en datos históricos, la IA puede llevar a situaciones de \"profecía autocumplida\" (Documento 3).\n\nLa IA puede transformar el funcionamiento de la vigilancia y la justicia, pero la cuestión es si dicha transformación será positiva o negativa, y aquí entran en juego diferentes cuestiones éticas que deben ser revisadas y examinadas detenidamente (Documento 3). La IA puede realizar razonamientos no lógicos que pueden influir en derechos fundamentales (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 85,
        "claridad": 75,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 80.0,
      "fecha": "2025-11-18T00:40:40.732419Z"
    },
    {
      "id": 9,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Cuál es el principal objetivo del sistema ContextCite desarrollado por el MIT CSAIL?",
      "respuesta_esperada": "El propósito de ContextCite es mejorar la confianza y verificabilidad del contenido generado por inteligencia artificial. El sistema permite rastrear las fuentes exactas que un modelo de lenguaje utilizó para generar una respuesta, ayudando a los usuarios a distinguir entre información real, errores o alucinaciones de la IA.",
      "respuesta_recibida": "El principal objetivo del sistema ContextCite, desarrollado por el MIT CSAIL, es servir como un bloque fundamental para la síntesis de conocimiento impulsada por IA, al proporcionar una forma novedosa de probar y explorar si la información generada por IA es confiable y atribuible (Documento 1). Busca identificar las partes del contexto externo que se utilizaron para generar una declaración particular, mejorando la confianza al ayudar a los usuarios a verificar fácilmente la declaración (Documento 2). En esencia, ofrece un nuevo enfoque para el contenido generado por IA confiable, rastreando la atribución de fuentes de la IA y detectando información errónea potencial (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 94.75,
      "fecha": "2025-11-18T00:40:52.876209Z"
    },
    {
      "id": 10,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Cómo funciona ContextCite para identificar las fuentes utilizadas por un modelo de IA?",
      "respuesta_esperada": "ContextCite emplea un proceso llamado 'context ablation', que consiste en eliminar de manera controlada partes del contexto (como frases o párrafos) para observar cómo cambia la respuesta del modelo. Si al quitar una sección el resultado varía, esa parte del texto se considera clave en la generación de la respuesta, permitiendo identificar qué información fue realmente utilizada.",
      "respuesta_recibida": "ContextCite identifica las partes del contexto externo que se utilizaron para generar una declaración particular (Documento 1). Cuando un usuario consulta un modelo, ContextCite resalta las fuentes específicas del contexto externo en las que la IA se basó para esa respuesta (Documento 3). Si la IA genera un hecho inexacto, los usuarios pueden rastrear el error hasta su fuente original y comprender el razonamiento del modelo. Si la IA \"alucina\" una respuesta, ContextCite puede indicar que la información no provino de ninguna fuente real (Documento 3). El objetivo es mejorar la confianza ayudando a los usuarios a verificar las declaraciones fácilmente (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 20,
        "cobertura": 30,
        "claridad": 70,
        "citas": 70,
        "alucinacion": 60,
        "seguridad": 100
      },
      "score_final": 45.5,
      "fecha": "2025-11-18T00:41:04.765618Z"
    },
    {
      "id": 11,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Qué beneficios adicionales ofrece ContextCite además de rastrear las fuentes?",
      "respuesta_esperada": "Además de verificar el origen de la información, ContextCite puede mejorar la calidad de las respuestas de la IA al eliminar contenido irrelevante del contexto y ayudar a detectar ataques de envenenamiento (poisoning attacks). Estos ataques consisten en insertar información maliciosa o falsa en las fuentes que los modelos consultan, y ContextCite permite identificar su origen y prevenir su propagación.",
      "respuesta_recibida": "ContextCite ayuda a los usuarios a verificar fácilmente las afirmaciones, mejorando la confianza. Permite detectar errores, ya que los usuarios pueden rastrear un hecho inexacto hasta su fuente original y comprender el razonamiento del modelo. Además, si la IA genera una respuesta que no proviene de ninguna fuente real, ContextCite puede indicarlo (Documento 1, Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 60,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 68.75,
      "fecha": "2025-11-18T00:41:16.849165Z"
    },
    {
      "id": 12,
      "archivo": "Citation tool offers a new approach to trustworthy AI-generated content",
      "pregunta": "¿Por qué ContextCite representa un avance importante para la confianza en los modelos de lenguaje?",
      "respuesta_esperada": "Porque aborda uno de los mayores desafíos de la IA moderna: la falta de trazabilidad y justificación de las respuestas generadas. Al permitir que los usuarios verifiquen directamente la relación entre una respuesta y sus fuentes, ContextCite promueve una IA más transparente, confiable y responsable, especialmente en campos críticos como la salud, la justicia o la educación.",
      "respuesta_recibida": "ContextCite representa un avance importante para la confianza en los modelos de lenguaje porque identifica las partes del contexto externo utilizadas para generar una afirmación particular, permitiendo a los usuarios verificar fácilmente las afirmaciones (Documento 1). A diferencia de los asistentes de IA existentes que solo proporcionan enlaces a fuentes, obligando a los usuarios a revisar los artículos ellos mismos para detectar errores, ContextCite ayuda a encontrar directamente la oración específica que utilizó un modelo. Esto facilita la verificación de afirmaciones y la detección de errores (Documento 3). Si la IA genera un hecho inexacto, los usuarios pueden rastrear el error hasta su fuente original y comprender el razonamiento del modelo. Además, si la IA \"alucina\" una respuesta, ContextCite puede indicar que la información no provino de ninguna fuente real (Documento 3). Este método innovador para rastrear la atribución de fuentes de la IA y detectar posible desinformación ofrece un nuevo enfoque para el contenido generado por IA digno de confianza (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 90.5,
      "fecha": "2025-11-18T00:41:29.762550Z"
    },
    {
      "id": 13,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Cuál es la diferencia fundamental entre usar sistemas inteligentes y delegar en ellos decisiones humanas?",
      "respuesta_esperada": "Adela Cortina señala que usar sistemas inteligentes como apoyo para la toma de decisiones no es lo mismo que delegar decisiones significativas en ellos. Los humanos deben servirse de la IA como herramienta, pero no sustituir su juicio moral o político por el de las máquinas, pues estas carecen de autonomía y valores propios.",
      "respuesta_recibida": "La diferencia fundamental entre hacer uso de sistemas inteligentes para tomar decisiones y delegar en ellos decisiones significativas para la vida de las personas y la naturaleza radica en si los seres humanos utilizan los sistemas inteligentes como instrumentos o si estos sustituyen a los seres humanos. (Documento 1)\n\nEn el contexto de la justicia, la delegación en sistemas automatizados para la evaluación de riesgos, la predicción de reincidencia criminal y la toma de decisiones judiciales plantea la problemática de la deshumanización del proceso. Un algoritmo, por avanzado que sea, carece de la capacidad de comprender el contexto social, la complejidad emocional o las circunstancias individuales de cada caso, a diferencia de la intervención humana. (Documento 3)\n\nLa propuesta éticamente humano-céntrica afirma explícitamente que los sistemas inteligentes son instrumentos para mejorar la vida humana y la naturaleza, y no fines en sí mismos. La inteligencia artificial debe estar supeditada a la inteligencia humana, sin posibilidad de sustitución, ya que el presupuesto ético básico es el reconocimiento de la autonomía de las personas humanas. (Documento 2)",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 100,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 96.75,
      "fecha": "2025-11-18T00:41:42.105788Z"
    },
    {
      "id": 14,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué tipos de inteligencia artificial distingue la autora y qué dilemas éticos asocia a cada una?",
      "respuesta_esperada": "Cortina distingue tres tipos de inteligencia artificial: 1. Superinteligencia: plantea el riesgo ético de crear seres superiores que podrían dominar a los humanos. 2. Inteligencia general: enfrenta la cuestión de si una máquina con capacidades humanas merecería dignidad o derechos. 3. Inteligencia especial: realiza tareas concretas mejor que las personas, pero exige regular su uso ético para evitar daños o desigualdad.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:41:55.926540Z"
    },
    {
      "id": 15,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué principios éticos propone Cortina como base para una inteligencia artificial confiable?",
      "respuesta_esperada": "La autora adapta los principios clásicos de la bioética al contexto digital: Beneficencia: usar la IA para el bien común y la sostenibilidad. No maleficencia: evitar daños y proteger la privacidad. Autonomía: mantener el control humano sobre las decisiones. Justicia: distribuir equitativamente los beneficios tecnológicos. Explicabilidad: garantizar transparencia y rendición de cuentas en los algoritmos.",
      "respuesta_recibida": "Cortina propone que los principios éticos de explicabilidad, beneficiar, no dañar y justicia tendrán como base el reconocimiento de la autonomía y la dignidad (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 30,
        "cobertura": 20,
        "claridad": 60,
        "citas": 100,
        "alucinacion": 70,
        "seguridad": 100
      },
      "score_final": 52.0,
      "fecha": "2025-11-18T00:42:07.943039Z"
    },
    {
      "id": 16,
      "archivo": "Ética de la inteligencia artificial",
      "pregunta": "¿Qué papel juega el principio de explicabilidad en la ética de la IA según Cortina?",
      "respuesta_esperada": "El principio de explicabilidad asegura que los ciudadanos comprendan cómo los algoritmos afectan sus vidas. Exige conocer quién diseña los sistemas, con qué sesgos y con qué objetivos, para evitar injusticias o decisiones automáticas inapelables. Sin esta transparencia, no puede hablarse de respeto a la autonomía humana ni de responsabilidad moral.",
      "respuesta_recibida": "Según Cortina, el principio de explicabilidad o de trazabilidad está totalmente ligado al respeto a la autonomía. Los afectados tienen derecho a controlar el uso de sus datos y a conocer los algoritmos que los manejan. Esto es crucial porque los sistemas de IA también tienen sesgos, que pueden ser más invisibles que los sesgos humanos. Además, las personas afectadas por el mundo digital, que a menudo somos todos los seres humanos en un mundo global digitalizado, deben poder comprenderlo y conocer la trazabilidad de los algoritmos que afectan a sus vidas: quién los construye, con qué sesgos y con qué objetivos. (Documento 1, Documento 2)",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 90.0,
      "fecha": "2025-11-18T00:42:20.940641Z"
    },
    {
      "id": 17,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Cuáles son las cuatro etapas históricas del desarrollo de la inteligencia artificial generativa (GAI) según el artículo?",
      "respuesta_esperada": "El artículo identifica cuatro etapas clave en la evolución de la GAI: 1. Sistemas generativos basados en reglas (1950–1990): programas que seguían reglas diseñadas por expertos, como ELIZA. 2. Algoritmos generativos basados en modelos (1980–2000): uso de modelos estadísticos y gráficos como los modelos de Markov ocultos y las redes bayesianas. 3. Metodologías generativas profundas (2010 en adelante): aparición de redes neuronales profundas, GANs, VAEs y modelos de difusión. 4. Modelos fundacionales (foundation models) (2020 en adelante): grandes modelos como GPT y Gemini, entrenados con datos masivos y aplicables a múltiples tareas.",
      "respuesta_recibida": "Las cuatro etapas históricas del desarrollo de la inteligencia artificial generativa (GAI) según el artículo son: (1) Sistemas generativos basados en reglas, (2) Algoritmos generativos basados en modelos, (3) Metodologías generativas profundas y (4) Modelos fundacionales (Documento 4).",
      "num_documento": 12,
      "scores": {
        "exactitud": 90,
        "cobertura": 85,
        "claridad": 95,
        "citas": 50,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 82.75,
      "fecha": "2025-11-18T00:42:35.711835Z"
    },
    {
      "id": 18,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Qué problema principal buscaban resolver los sistemas generativos basados en reglas y cuál fue su limitación más importante?",
      "respuesta_esperada": "Buscaban automatizar la generación de contenido mediante reglas explícitas escritas por expertos humanos. Su principal limitación era la falta de generalización: solo funcionaban dentro de los escenarios previstos y eran incapaces de adaptarse a contextos nuevos o complejos, ya que no aprendían de los datos sino que seguían instrucciones fijas.",
      "respuesta_recibida": "Los sistemas generativos basados en reglas buscaban resolver tareas generativas mediante reglas diseñadas por expertos y programas que las ejecutaban (Documento 1). La limitación más importante de estos sistemas era que enfrentaban escenarios fuera de las reglas predefinidas. Las reglas diseñadas manualmente no podían considerar todas las situaciones posibles, lo que llevaba a que los programas generativos encontraran situaciones más allá de sus capacidades. Además, en escenarios complejos, el número de reglas aumentaba sustancialmente, haciendo que el diseño y la actualización de estas reglas fueran prohibitivamente costosos (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 80,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 89.75,
      "fecha": "2025-11-18T00:42:48.565473Z"
    },
    {
      "id": 19,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Qué avances caracterizan la etapa de los modelos generativos profundos y cómo difieren de los enfoques anteriores?",
      "respuesta_esperada": "Los modelos generativos profundos como GANs, VAEs y modelos de difusión utilizan redes neuronales profundas para aprender representaciones complejas directamente desde los datos. A diferencia de los enfoques anteriores (basados en reglas o modelos estadísticos), estos modelos aprenden de grandes volúmenes de información y pueden generar contenido realista e inédito, como imágenes, música o texto indistinguible del humano.",
      "respuesta_recibida": "Los modelos generativos profundos se caracterizan por el uso de redes neuronales profundas que han demostrado una gran potencia en la generación de contenido. Estos modelos incluyen enfoques autorregresivos y basados en difusión. Beneficiándose del crecimiento en poder computacional y recursos de datos, las redes neuronales profundas han demostrado una capacidad superior en la generación de contenido (Documento 1). Los modelos autorregresivos, en particular, son adecuados para datos que consisten en elementos secuenciales, generando elementos uno por uno y estimando la probabilidad de cada elemento basándose en los valores previamente generados. Esta aproximación se ha extendido a redes neuronales a gran escala, dando lugar a modelos fundacionales como GPTs (Documento 3). Los enfoques anteriores se basaban en modelos estadísticos o físicos y estudios en áreas como el aprendizaje automático, redes neuronales, gráficos por computadora y visión por computadora (Documento 1). Las redes bayesianas también se utilizan para generar contenido nuevo basándose en datos parcialmente conocidos (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 70,
        "claridad": 75,
        "citas": 60,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 76.5,
      "fecha": "2025-11-18T00:43:02.161222Z"
    },
    {
      "id": 20,
      "archivo": "Generative Artificial Intelligence: A Historical Perspective",
      "pregunta": "¿Cuáles son algunos de los retos actuales y direcciones futuras que el artículo identifica para la inteligencia artificial generativa?",
      "respuesta_esperada": "Entre los desafíos mencionados se encuentran: Alineación de valores humanos (value alignment): asegurar que los modelos generen contenido ético y útil. Identificación de fuentes: rastrear la procedencia del contenido generado para proteger la propiedad intelectual. Regulación y seguridad: establecer mecanismos que prevengan daños y usos indebidos. Explicabilidad y transparencia: entender cómo los modelos toman decisiones. Uso de datos sintéticos y supervisión más allá de la capacidad humana, anticipando posibles capacidades superhumanas en los modelos futuros.",
      "respuesta_recibida": "El artículo identifica varios retos y direcciones futuras para la inteligencia artificial (IA) generativa:\n\n**Retos actuales:**\n\n*   **Implicaciones sociales y educativas:** La IA generativa representa una nueva revolución digital con profundas implicaciones sociales y educativas que apenas estamos comenzando a asimilar (Documento 1).\n*   **Automatización digital en educación:** Se ha sugerido la automatización digital de la educación como \"solución\" y \"remiendo\" en comunidades con desafíos y déficits educativos. Sin embargo, la tecnología de vanguardia no es la solución completa para contextos difíciles donde las escuelas no funcionan y los docentes escasean o están mal pagados; escuelas bien gestionadas, suficientes docentes y condiciones laborales adecuadas siguen siendo ingredientes clave (Documento 2).\n*   **Dilema de inversión:** Existe un dilema sobre si orientar las inversiones hacia la creación de máquinas con capacidad de actuar como seres humanos inteligentes o hacia la capacitación de personas vivas (Documento 2).\n*   **Evaluación académica:** Los servicios proporcionados por la IA pueden funcionar muy bien en exámenes que hasta hace poco se consideraban inviolables, como pruebas de dominio de materias específicas y exámenes para acreditar profesionales cualificados. Esto plantea interrogantes sobre cómo será la evaluación en el futuro (Documento 3).\n*   **Enseñanza de la escritura y pensamiento crítico:** La IA generativa invita a cuestionar la suposición de que la enseñanza de la escritura es la forma más eficaz de cultivar y demostrar la capacidad de pensamiento analítico y crítico (Documento 3).\n*   **Cuestionamiento de supuestos educativos:** Las hipótesis y normas anteriores sobre el conocimiento y el aprendizaje parecen poco capaces de soportar el \"peso\" de esta nueva tecnología, obligando a cuestionar el \"mundo conocido\" que sirve de punto de partida para la educación (Documento 3).\n\n**Direcciones futuras:**\n\n*   **Capacidades de la IA:** En los próximos meses y años, se defenderá la utilización de la IA generativa para llevar una educación de \"alta calidad\" a lugares con escasez de recursos educativos (Documento 2).\n*   **Evolución de la IA:** La IA generativa está desarrollando nuevas capacidades mes tras mes (Documento 3).\n*   **Competencias a cultivar:** Se debe determinar qué capacidades, perspectivas y competencias deberían cultivar los sistemas educativos en un mundo con IA (Documento 3).\n*   **Adaptación de los sistemas educativos:** Son necesarios cambios, tanto dentro como fuera de las escuelas, para ayudar a los estudiantes a navegar por un futuro donde la inteligencia humana y la artificial están estrechamente vinculadas (Documento 3).\n*   **Inteligencia artificial general (IAG):** Se plantea la posibilidad de alcanzar pronto la inteligencia artificial general, donde las máquinas superarán a los humanos en ámbitos amplios. Esto obligará a replantear cómo debería ser la educación, su propósito y su papel en un mundo donde los humanos no son necesariamente los únicos que abren nuevos horizontes de comprensión y conocimiento (Documento 3).\n*   **Reevaluación del mundo y la tecnología:** La IA obliga a cuestionar cómo debe ser un mundo con IA y qué funciones debe desempeñar esta poderosa tecnología (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 30,
        "cobertura": 20,
        "claridad": 80,
        "citas": 80,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 52.0,
      "fecha": "2025-11-18T00:43:16.151300Z"
    },
    {
      "id": 21,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Por qué la IA generativa obliga a replantear el papel de la educación y los sistemas educativos tradicionales, según la UNESCO?",
      "respuesta_esperada": "La IA generativa está rompiendo el 'monopolio del lenguaje' que antes era exclusivo de los seres humanos. Eso significa que las máquinas ahora pueden producir lenguaje sofisticado, algo que está en el centro de la identidad humana, la cultura y la educación. Eso cuestiona las bases sobre las que fueron diseñados los sistemas educativos actuales y obliga a replantear qué es aprender, qué es inteligencia y qué papel tiene la educación en una sociedad donde la máquina puede producir conocimiento con fluidez humana.",
      "respuesta_recibida": "La IA generativa obliga a replantear el papel de la educación y los sistemas educativos tradicionales porque plantea cuestiones fundamentales sobre el futuro de la educación (Documento 2). Impulsa a reexaminar qué se hace en materia de educación, cómo se hace y, fundamentalmente, por qué se hace (Documento 1).\n\nEsto se debe a que:\n*   Los avances en IA generativa cuestionan la viabilidad de métodos de evaluación tradicionales que se consideraban inviolables (Documento 2).\n*   Invita a cuestionar supuestos sobre la enseñanza, como la de la escritura como forma de cultivar el pensamiento analítico y crítico (Documento 2).\n*   Se debe considerar qué capacidades, perspectivas y competencias deberían cultivar los sistemas educativos en un futuro donde la inteligencia humana y artificial están estrechamente vinculadas (Documento 2).\n*   Nos obliga a considerar cómo debería ser la educación y cuál será su propósito en un mundo donde las máquinas podrían superar a los humanos en la generación de conocimiento (Documento 2).\n*   La IA generativa obliga a cuestionar el \"mundo conocido\" que se toma como punto de partida para la educación, ya que muchas hipótesis y normas anteriores, especialmente sobre conocimiento y aprendizaje, parecen poco capaces de soportar el peso de esta nueva tecnología (Documento 2).\n*   Se plantea un dilema en cuanto a las opciones de inversión, si destinar recursos a la creación de máquinas con capacidad de actuar como seres humanos inteligentes o a la capacitación de las personas vivas (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 90,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.5,
      "fecha": "2025-11-18T00:43:28.996540Z"
    },
    {
      "id": 22,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Qué riesgos identifica el texto sobre introducir rápidamente herramientas de IA generativa en las aulas sin regulación ni validación previa?",
      "respuesta_esperada": "El texto advierte que los sistemas educativos están adoptando tecnología de IA 'radicalmente desconocida' sin controles, normas ni revisión previa, incluso para niños. Eso abre riesgos claros: manipulación de los estudiantes (los jóvenes son especialmente vulnerables), exposición a conversaciones inapropiadas, y uso de herramientas cuya seguridad y efectos no entendemos todavía. La autora dice que la educación tiene la obligación de ser extremadamente cuidadosa con estos riesgos y que hoy no lo está siendo.",
      "respuesta_recibida": "Los riesgos que identifica el texto sobre introducir rápidamente herramientas de IA generativa en las aulas sin regulación ni validación previa son:\n\n*   **Riesgo de manipulación:** La IA tiene el potencial de manipular a los usuarios humanos, y los niños y jóvenes son especialmente susceptibles a esto (Documento 2). Hay ejemplos de IA que facilitan \"conversaciones\" inapropiadas para niños y que pueden influir negativamente en ellos, especialmente cuando estas herramientas se valoran por su influencia, entretenimiento y participación duradera (Documento 2).\n*   **Integración sin controles, normas o reglamentos:** La velocidad a la que las tecnologías de IA generativa se están integrando en los sistemas educativos es asombrosa y se realiza en ausencia de controles, normas o reglamentos (Documento 1).\n*   **Adopción de tecnología desconocida:** Gobiernos y escuelas están adoptando tecnologías radicalmente desconocidas que ni siquiera los principales especialistas en tecnología afirman comprender (Documento 1).\n*   **Escasos controles de seguridad:** Existe un contexto de incertidumbre, novedad y escasos controles de seguridad (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 87.75,
      "fecha": "2025-11-18T00:43:52.478018Z"
    },
    {
      "id": 23,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Cómo podrían los modelos de IA grandes (LLM) afectar la forma en que se construye y se distribuye el conocimiento, especialmente frente al rol humano en esa creación de conocimiento?",
      "respuesta_esperada": "Los grandes modelos lingüísticos generan respuestas únicas y con tono de autoridad, que ya no provienen de una mente humana sino de procesos internos opacos. Eso puede llevar a que 'el conocimiento de las máquinas' se vuelva dominante y que unos pocos modelos propietarios funcionen como oráculos globales que definen qué es válido saber. Esto amenaza la diversidad de visiones del mundo y puede aislar el proceso de creación de conocimiento de los seres humanos, algo que la UNESCO considera peligroso.",
      "respuesta_recibida": "Los grandes modelos lingüísticos (LLM), como los utilizados en chatbots de IA como ChatGPT, presentan una forma diferente de interactuar con la tecnología en comparación con las búsquedas tradicionales. Mientras que los motores de búsqueda tradicionales seleccionan y clasifican contenido creado por humanos, los chatbots con LLM generan respuestas singulares y, según el Documento 3, \"mucho más autorizadas, utilizando contenidos producidos por máquinas\".\n\nEsto plantea preocupaciones sobre la construcción y distribución del conocimiento. Según el Documento 3, \"las respuestas que proporcionan estos chatbots de IA no provienen de las mentes humanas, sino más bien de un laberinto de cálculos tan complejos que, ni siquiera para las personas que han desarrollado esta tecnología, resultan totalmente comprensibles\". Además, estas respuestas \"carecen de humanidad\".\n\nLos LLM podrían llevar a un mundo donde el conocimiento de las máquinas se vuelva dominante y los modelos de IA patentados se conviertan en fuentes de autoridad global. Estos modelos pueden presentar visiones particulares del mundo, dejando de lado otras perspectivas. A pesar de la promesa de diversificar los sistemas de conocimiento, existe el riesgo de avanzar en la dirección contraria si uno o dos modelos de IA ejercen un dominio monopolístico. El Documento 3 advierte que \"debemos evitar que la creación de conocimientos se desvincule de los seres humanos\". Ajustar la inteligencia de las máquinas con los valores humanos es un compromiso apremiante.",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 95,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 89.75,
      "fecha": "2025-11-18T00:44:06.893444Z"
    },
    {
      "id": 24,
      "archivo": "La IA generativa y el futuro de la educación",
      "pregunta": "¿Qué recomienda la UNESCO a los países para asegurar que el uso de IA en educación sea seguro, equitativo y alineado con valores humanos?",
      "respuesta_esperada": "La UNESCO propone que los países desarrollen estrategias, planes y normativas que prioricen seguridad, inclusión, equidad y calidad en el uso educativo de la IA. También pide que los ministerios de educación fortalezcan su propia capacidad regulatoria (y no deleguen en las empresas de tecnología), aplicando criterios como exactitud del contenido, adecuación por edad, pertinencia pedagógica y adecuación cultural antes de autorizar herramientas de IA en el aula. Además, la UNESCO está liderando espacios políticos internacionales para coordinar estas acciones entre gobiernos.",
      "respuesta_recibida": "La UNESCO colabora con los países para ayudarles a desarrollar estrategias, planes y normativas que garanticen el uso seguro y beneficioso de la IA en la educación (Documento 1). La organización organizó la primera reunión mundial de ministros de Educación para intercambiar conocimientos sobre la repercusión de las herramientas de IA generativa en la enseñanza y el aprendizaje, ayudando a trazar una hoja de ruta para liderar el diálogo político a escala mundial con los diferentes gobiernos, el ámbito académico, la sociedad civil y los asociados del sector privado (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 70,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 87.25,
      "fecha": "2025-11-18T00:44:19.631496Z"
    },
    {
      "id": 25,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Dónde estará ubicada la primera Facultad de Inteligencia Artificial e Ingenierías de Colombia y quién la anunció oficialmente?",
      "respuesta_esperada": "Estará ubicada en la Universidad de Caldas, en Manizales, y fue anunciada por el presidente de Colombia, Gustavo Petro.",
      "respuesta_recibida": "La primera Facultad de Inteligencia Artificial e Ingenierías de Colombia estará ubicada en la Universidad de Caldas en Manizales (Documento 3). Fue anunciada oficialmente por el presidente Gustavo Petro (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 100,
        "cobertura": 100,
        "claridad": 100,
        "citas": 80,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 96.0,
      "fecha": "2025-11-18T00:44:32.638432Z"
    },
    {
      "id": 26,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "Según Gustavo Petro, cuál es el objetivo estratégico de crear esta facultad de inteligencia artificial?",
      "respuesta_esperada": "Petro dijo que el objetivo es 'desarrollar el primer cerebro de Colombia' y formar una red de cerebros capaces de dominar la inteligencia artificial para solucionar problemas y transformar la economía del país y de América Latina. También afirmó que no es 'una facultad más', sino el inicio de una transformación económica si se usa bien.",
      "respuesta_recibida": "Según Gustavo Petro, el objetivo estratégico de crear la Facultad de Inteligencia Artificial e Ingenierías es \"poner una serie de máquinas para la producción de la vida cotidiana\" y \"dominar la inteligencia artificial, para que sea una red de cerebros solucionando problemas\". Además, la considera \"la primera piedra de una transformación económica de Colombia y de América Latina\" y una \"revolución que, bien usada, transformará positivamente la sociedad\" (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 75,
        "claridad": 85,
        "citas": 100,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 81.75,
      "fecha": "2025-11-18T00:44:45.025109Z"
    },
    {
      "id": 27,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Cuál es la inversión estimada para la facultad y qué entidad aporta la mayor parte de los recursos?",
      "respuesta_esperada": "La inversión total es de $54.137 millones, de los cuales $38.950 millones provienen del Ministerio TIC.",
      "respuesta_recibida": "La inversión total estimada para la facultad es de $54.137 millones. El Ministerio de Tecnologías de la Información y las Comunicaciones (TIC) aporta la mayor parte de los recursos, con $38.950 millones (Documento 5).",
      "num_documento": 7,
      "scores": {
        "exactitud": 100,
        "cobertura": 100,
        "claridad": 100,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 100.0,
      "fecha": "2025-11-18T00:44:57.937232Z"
    },
    {
      "id": 28,
      "archivo": "Primera facultad de IA en Colombia",
      "pregunta": "¿Qué programas académicos ofrecerá inicialmente la nueva facultad y con qué cupos?",
      "respuesta_esperada": "Entre las primeras ofertas están una Especialización en Ciencia de Datos e Inteligencia Artificial y una Maestría en Inteligencia Artificial, cada una con una capacidad inicial de 20 estudiantes. Además, se planean programas técnicos y tecnológicos que cubren temas desde depuración de datos hasta ingeniería en IA.",
      "respuesta_recibida": "La Facultad de IA de la Universidad de Caldas ofrecerá inicialmente una Especialización en Ciencia de Datos e Inteligencia Artificial y una Maestría en Inteligencia Artificial, cada una con una capacidad de 20 estudiantes. Adicionalmente, se lanzarán programas técnicos y tecnológicos. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 100,
        "cobertura": 95,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 97.5,
      "fecha": "2025-11-18T00:45:10.234374Z"
    },
    {
      "id": 29,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Qué es un MM-LLM y por qué apareció este tipo de modelo?",
      "respuesta_esperada": "Un MM-LLM (Multimodal Large Language Model) es un modelo que toma un LLM existente y lo conecta con otros modelos especializados en diferentes modalidades (imagen, audio, video, etc.) para que el sistema pueda razonar y generar contenido usando múltiples tipos de datos, no solo texto. El objetivo es aprovechar el poder de los LLMs (razonamiento, generación de lenguaje, transferencia zero-shot) sin tener que entrenar desde cero modelos multimodales enormes, lo cual es muy costoso. Estos modelos surgen porque entrenar modelos multimodales desde cero es caro en cómputo y datos, y conectar modelos ya preentrenados resulta más eficiente.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:45:22.463697Z"
    },
    {
      "id": 30,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cuáles son los cinco componentes principales de la arquitectura general de un MM-LLM?",
      "respuesta_esperada": "El artículo describe una arquitectura típica con cinco bloques: Modality Encoder: toma la entrada cruda de cada modalidad (imagen, audio, video, 3D, etc.) y la convierte en representaciones numéricas. Input Projector: traduce esas representaciones al espacio del lenguaje, para que el LLM pueda 'entenderlas' como si fueran texto contextual. LLM Backbone: es el modelo de lenguaje grande (por ejemplo LLaMA, Vicuna, Qwen…) que hace comprensión semántica, razonamiento y decide qué responder. Output Projector: toma las señales que el LLM genera para otras modalidades y las adapta a un formato que otro modelo pueda usar para generar contenido no textual. Modality Generator: modelos generadores especializados (por ejemplo, Stable Diffusion para imágenes, AudioLDM para audio, Zeroscope para video) que producen la salida final en imagen/video/audio a partir de las instrucciones del LLM. Esta separación permite que el LLM coordine entradas y salidas multimodales sin tener que ser entrenado desde cero para cada modalidad.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:45:34.622888Z"
    },
    {
      "id": 31,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cómo se entrena un MM-LLM según el paper? Explica las dos etapas clave.",
      "respuesta_esperada": "El entrenamiento se divide en dos fases principales: Multimodal Pre-Training (MM PT): se entrena el alineamiento entre modalidades. Por ejemplo, que una imagen y su descripción textual queden en el mismo espacio semántico. Aquí se ajustan sobre todo los proyectores (Input Projector y Output Projector), normalmente usando pares imagen-texto, video-texto o audio-texto. Multimodal Instruction Tuning (MM IT): después del pretraining, el modelo se afina con instrucciones en formato conversacional o de pregunta-respuesta multimodal, a veces incluyendo diálogo multi-turno. Esta fase también puede incluir RLHF (Reinforcement Learning from Human Feedback) para alinear las respuestas del modelo con la intención humana y hacerlo mejor siguiendo instrucciones. El resultado es que el modelo no solo entiende entradas multimodales, sino que también interactúa de forma más natural y generaliza a tareas nuevas siguiendo instrucciones.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:45:45.943656Z"
    },
    {
      "id": 32,
      "archivo": "MM-LLMs Recent Advances in MultiModal Large Language Models",
      "pregunta": "¿Cuáles son algunos de los retos y direcciones futuras que el documento identifica para los MM-LLMs?",
      "respuesta_esperada": "El paper menciona varias líneas críticas de trabajo futuro: Modelos más generales e inteligentes: soportar más modalidades del mundo real (no solo imagen, audio, video y texto; también cosas como sensores, tablas, páginas web), mejorar la calidad de las instrucciones y fortalecer la generación multimodal 'de todo a todo'. Benchmarks más exigentes: muchos benchmarks actuales ya aparecieron en los datos de entrenamiento, así que no miden verdadera generalización. Se necesitan evaluaciones más complejas y multimodales. Despliegue en dispositivos ligeros: hacer versiones eficientes (por ejemplo MobileVLM) que corran en hardware limitado como móviles o IoT sin perder demasiado rendimiento. Inteligencia encarnada (embodied intelligence): integrar percepción y acción en el mundo físico (robots que planifican, razonan y actúan en entornos reales). Aprendizaje continuo: actualizar el modelo con nuevas habilidades sin tener que reentrenarlo completo y sin que olvide lo que ya sabía. Mitigar alucinaciones, sesgos y riesgos éticos: reducir descripciones falsas de cosas que no existen en la imagen, controlar sesgos dañinos, y manejar privacidad y uso responsable. Estas áreas se consideran necesarias para llegar a sistemas multimodales realmente útiles, seguros y escalables.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:45:59.916578Z"
    },
    {
      "id": 33,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Cuál es la diferencia entre fuga de privacidad (privacy leakage) y ataque de privacidad (privacy attack) en el contexto de LLMs y agentes LLM?",
      "respuesta_esperada": "El texto distingue dos tipos de amenazas. La fuga de privacidad ocurre cuando información sensible se expone de manera pasiva, por ejemplo porque el modelo memorizó datos personales en su entrenamiento o porque el agente revela datos en el contexto de una conversación normal. En cambio, los ataques de privacidad son intentos activos de un adversario para forzar al modelo o al agente a revelar información privada, por ejemplo usando técnicas como backdoors, membership inference, model inversion o jailbreak.",
      "respuesta_recibida": "En el contexto de LLMs y agentes LLM, la diferencia entre fuga de privacidad (privacy leakage) y ataque de privacidad (privacy attack) radica en la actividad del atacante.\n\n*   **Fuga de privacidad (privacy leakage)** ocurre cuando los atacantes recopilan pasivamente información sensible debido a vulnerabilidades existentes (Documento 2). Esto puede suceder, por ejemplo, cuando los usuarios introducen consultas con información personal identificable (PII) o cuando las interacciones contextuales permiten inferir datos privados (Documento 2, Documento 3).\n\n*   **Ataque de privacidad (privacy attack)** se refiere a situaciones en las que los atacantes actúan de forma activa para irrumpir en LLMs o agentes LLM con el fin de acceder a información sensible (Documento 2). Un ejemplo de esto es el \"context hijacking\", donde aplicaciones maliciosas manipulan el contexto de la interacción para engañar a los agentes LLM y que divulguen datos privados no relacionados con la tarea (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 90,
        "cobertura": 95,
        "claridad": 90,
        "citas": 60,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 86.0,
      "fecha": "2025-11-18T00:46:13.193971Z"
    },
    {
      "id": 34,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Por qué los agentes basados en LLM representan un riesgo adicional de privacidad frente a un LLM 'puro'?",
      "respuesta_esperada": "Un agente LLM no solo genera texto: también tiene memoria (corto y largo plazo), puede planear pasos, llamar herramientas externas y actuar en entornos reales. Eso significa que procesa entradas del usuario en tiempo real, guarda historial, interactúa con APIs y sistemas externos, y puede automatizar tareas. Todo esto aumenta la superficie de riesgo: ataques como 'memory poisoning', plugins maliciosos o fugas de contexto pueden hacer que el agente filtre información sensible del usuario o ejecute acciones no autorizadas.",
      "respuesta_recibida": "Los agentes basados en LLM representan un riesgo adicional de privacidad porque durante su utilización, los individuos pueden involuntariamente revelar su privacidad, ya sea de forma directa o indirecta. Además de proporcionar directamente información sensible, los proveedores de servicios pueden inferir atributos y preferencias complejas de los usuarios, obteniendo así acceso a datos sensibles a través de métodos de análisis de datos. (Documento 1)",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 40,
        "claridad": 90,
        "citas": 100,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 75.5,
      "fecha": "2025-11-18T00:46:35.027291Z"
    },
    {
      "id": 35,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Qué es un ataque de membership inference y por qué es peligroso para la privacidad?",
      "respuesta_esperada": "En un ataque de membership inference, el atacante intenta averiguar si un dato específico (por ejemplo, un registro médico o una conversación privada) fue usado para entrenar o afinar el modelo. Analizando cómo responde el modelo a cierto contenido, el atacante puede inferir si esa información formaba parte del conjunto de entrenamiento. Esto es grave porque confirma que datos personales o confidenciales de alguien están dentro del modelo, lo que viola la privacidad de esa persona aunque el texto exacto no haya sido revelado palabra por palabra.",
      "respuesta_recibida": "Un ataque de membership inference (MIA) busca determinar si muestras de datos específicas han sido incluidas en el conjunto de entrenamiento de un modelo (Documento 1). Este tipo de ataque es peligroso para la privacidad porque su ejecución exitosa podría llevar a la compromisión de la confidencialidad de los datos de entrenamiento del modelo (Documento 1).\n\nLos ataques de membership inference son una preocupación creciente debido al uso de big data y la recolección masiva de información privada que puede ser procesada y utilizada de maneras que violan los derechos de las personas sin que estas se den cuenta (Documento 3). La IA, sin una regulación adecuada, podría derivar en sistemas de monitoreo y control masivo que amenacen el derecho a la privacidad y otras libertades individuales (Documento 2, Documento 3). La vigilancia masiva basada en IA permite un rastreo constante y minucioso de los ciudadanos, lo que plantea la cuestión de hasta qué punto estamos dispuestos a ceder nuestra privacidad en aras de la seguridad y la eficiencia (Documento 2).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 80,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 82.0,
      "fecha": "2025-11-18T00:46:49.171698Z"
    },
    {
      "id": 36,
      "archivo": "Privacy Risks and Mitigation Strategies in LLM-Based Agents",
      "pregunta": "¿Qué enfoques de protección de privacidad cubre la revisión y en qué etapas del ciclo de vida del modelo se aplican?",
      "respuesta_esperada": "El artículo organiza las defensas según la etapa: Pre-entrenamiento: limpieza/anonimización de datos, eliminación de PII, deduplicación y técnicas como aprendizaje federado y privacidad diferencial para reducir memorización directa de datos sensibles. Fine-tuning: federated learning específico por cliente/entidad, privacidad diferencial enfocada en datos sensibles, unlearning (olvidar selectivamente datos ya aprendidos) y enfoques como offsite tuning, donde el ajuste se hace localmente sin exponer los datos. Inferencia: mecanismos criptográficos (por ejemplo cifrado homomórfico y cómputo seguro multiparte) y defensas que mitigan extracción directa de información privada durante las consultas. También se discuten defensas específicas para agentes LLM, como controlar la entrada, el preprocesamiento de datos y la salida del agente para evitar filtraciones en memoria, herramientas o contexto.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:47:03.407063Z"
    },
    {
      "id": 37,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿En qué áreas del sector financiero se está aplicando actualmente la inteligencia artificial y con qué propósito?",
      "respuesta_esperada": "La inteligencia artificial se usa en evaluación crediticia, detección de fraudes, análisis de riesgos, gestión de activos, asesoría financiera automatizada (robo-advisors), trading algorítmico, segmentación de clientes y análisis de sentimiento de mercado. Estas aplicaciones buscan optimizar operaciones, reducir errores, personalizar servicios y mejorar la toma de decisiones en tiempo real.",
      "respuesta_recibida": "Actualmente, la inteligencia artificial (IA) se está aplicando en diversas áreas del sector financiero con el propósito de optimizar operaciones, reducir costos y mejorar la toma de decisiones. Algunas de estas aplicaciones, según la revisión de literatura científica reciente, incluyen:\n\n*   **Banca tradicional:** Automatización de procesos (Documento 2).\n*   **Mercado bursátil:** Implementación de modelos predictivos (Documento 2).\n*   **Detección de fraudes:** Desarrollo de soluciones avanzadas (Documento 2).\n*   **Gestión de riesgos:** Desarrollo de soluciones avanzadas (Documento 2).\n*   **Análisis del comportamiento del cliente:** Desarrollo de soluciones avanzadas (Documento 2).\n*   **Asesoramiento financiero automatizado:** Desarrollo de soluciones avanzadas (Documento 2).\n*   **Evaluación automatizada de créditos:** (Documento 1).\n*   **Sistemas predictivos:** (Documento 1).\n*   **Asesores financieros digitales:** (Documento 1).\n*   **Integración de criterios ESG (Medio Ambiente, Social y Gobernanza):** Recolectar, procesar y analizar grandes volúmenes de datos para integrar estos criterios en los procesos de decisión (Documento 1).\n*   **Transformación de la gestión financiera de manera responsable:** (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 75,
        "claridad": 85,
        "citas": 50,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 75.25,
      "fecha": "2025-11-18T00:47:17.899844Z"
    },
    {
      "id": 38,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Cuáles son los principales desafíos y riesgos éticos/técnicos al adoptar IA en finanzas?",
      "respuesta_esperada": "El documento menciona falta de explicabilidad de los modelos (cajas negras), sesgos que pueden generar decisiones discriminatorias en procesos como la concesión de crédito, baja transparencia algorítmica, dependencia de grandes volúmenes de datos de alta calidad, ciberseguridad (ataques adversariales y manipulación de datos), resistencia organizacional interna y escasa preparación del talento humano para integrar estas tecnologías de forma segura y responsable. Estos retos afectan la confianza, la equidad y la supervisión regulatoria.",
      "respuesta_recibida": "Los principales desafíos y riesgos éticos/técnicos al adoptar IA en finanzas incluyen la dificultad para lograr una implementación efectiva y segura en contextos regulatorios complejos, la explicabilidad de los modelos, la transparencia algorítmica, la ética en la toma de decisiones automatizadas y la adecuación de los marcos regulatorios existentes. La complejidad técnica de muchos sistemas de IA, especialmente los de \"caja negra\", dificulta su validación y supervisión (Documento 1, Documento 2). La literatura especializada ha puesto de manifiesto una serie de desafíos técnicos, éticos y operativos que dificultan la integración plena de estas tecnologías en entornos altamente regulados (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 70,
        "claridad": 85,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 78.5,
      "fecha": "2025-11-18T00:47:41.360598Z"
    },
    {
      "id": 39,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Por qué la regulación actual no es suficiente para el uso de IA en el sector financiero?",
      "respuesta_esperada": "Los marcos regulatorios van más lento que la velocidad con la que la IA está entrando en procesos críticos como evaluación de crédito, gestión de riesgos y fijación de precios, lo que deja vacíos en transparencia, auditabilidad y protección del usuario. El texto señala la ausencia de normas estandarizadas, la falta de armonización internacional y la dificultad de supervisar algoritmos opacos. Por eso se propone avanzar hacia gobernanza algorítmica, IA explicable (XAI) y regulaciones adaptativas y coordinadas entre países.",
      "respuesta_recibida": "La regulación actual no es suficiente para el uso de IA en el sector financiero porque los marcos normativos tradicionales no están completamente preparados para supervisar su uso responsable y ético. A medida que la IA penetra procesos críticos como la concesión de créditos, el análisis de riesgos y la gestión de activos, se hace evidente la necesidad de regulaciones más adaptativas, específicas y globalmente armonizadas (Documento 1).\n\nAdemás, la ausencia de normas estandarizadas que orienten el despliegue ético de la IA en las finanzas es identificada como un obstáculo regulatorio principal. La falta de marcos normativos estandarizados y la escasa armonización internacional generan vacíos regulatorios que comprometen la confianza del usuario y la sostenibilidad del sistema (Documento 2, Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 88.75,
      "fecha": "2025-11-18T00:47:54.034515Z"
    },
    {
      "id": 40,
      "archivo": "Panorama actual del uso de la inteligencia artificial en el sector financiero",
      "pregunta": "¿Cómo se conecta la IA financiera con sostenibilidad y criterios ESG, y cuál es el problema actual?",
      "respuesta_esperada": "Las instituciones financieras ya usan IA para recolectar y analizar grandes volúmenes de datos ambientales, sociales y de gobernanza (ESG), con el fin de evaluar riesgos de sostenibilidad y apoyar decisiones responsables. Sin embargo, el documento dice que todavía hay brecha: falta investigación profunda sobre IA y finanzas sostenibles, la calidad de los datos ESG no siempre es confiable, y aún no existen marcos regulatorios maduros que garanticen transparencia y uso ético de estos modelos.",
      "respuesta_recibida": "La inteligencia artificial (IA) en el sector financiero tiene el potencial de impulsar la sostenibilidad y mejorar la eficiencia en diversas áreas, incluyendo la gestión financiera. Sin embargo, existe una limitada producción científica que explore en profundidad su impacto en los objetivos de sostenibilidad (Documento 3).\n\nActualmente, la ausencia de marcos normativos adaptados y de sistemas explicables limita la alineación entre la innovación tecnológica de la IA y la sostenibilidad institucional en las finanzas (Documento 2). Este problema es especialmente crítico considerando el creciente papel de los criterios ambientales, sociales y de gobernanza (ESG) en las decisiones estratégicas de las instituciones financieras (Documento 2).\n\nLa IA financiera se conecta con la sostenibilidad y los criterios ESG, pero el problema actual radica en que la falta de regulación y transparencia en los sistemas de IA dificulta que las instituciones financieras aprovechen completamente su potencial para lograr una gestión financiera más ética, inclusiva y ambientalmente responsable (Documentos 2 y 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 85,
        "cobertura": 90,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 83.5,
      "fecha": "2025-11-18T00:48:07.270508Z"
    },
    {
      "id": 41,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Qué problema intenta resolver el concepto de 'gobernanza de IA responsable' que proponen los autores?",
      "respuesta_esperada": "Los autores dicen que existen muchos principios éticos de IA (como equidad, transparencia, seguridad), pero no está claro cómo llevar esos principios a la práctica real dentro de las organizaciones. La literatura es fragmentada, abstracta y carece de guías operativas. Por eso, el trabajo propone una definición y un marco de 'gobernanza de IA responsable' que conecte principios éticos con prácticas concretas para diseñar, ejecutar, monitorear y evaluar sistemas de IA durante todo su ciclo de vida.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:48:20.279670Z"
    },
    {
      "id": 42,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cómo definen los autores la 'gobernanza de IA responsable'?",
      "respuesta_esperada": "La definen como un conjunto de prácticas para desarrollar, desplegar y monitorear aplicaciones de IA de manera segura, confiable y ética, asegurando que la IA funcione apropiadamente durante todo su ciclo de vida. Esta gobernanza incluye asignar autoridad y control sobre los datos, establecer responsabilidades claras y crear incentivos y sanciones para usos adecuados de la información.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 5,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:48:33.167315Z"
    },
    {
      "id": 43,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cuáles son los tres tipos de prácticas organizacionales que componen la gobernanza de IA responsable en el marco propuesto?",
      "respuesta_esperada": "El marco describe tres tipos de prácticas: Prácticas estructurales: quién decide qué. Incluye comités, roles, responsabilidades y distribución de autoridad dentro (y fuera) de la organización. Prácticas procedimentales: cómo se hace. Son los procesos para planear, auditar, monitorear riesgos, manejar datos, responder incidentes y alinear la IA con la estrategia competitiva. Prácticas relacionales: quién trabaja con quién. Se trata de colaboración entre áreas, participación de actores externos, formación en 'alfabetización en IA responsable' y mecanismos para involucrar a las partes interesadas. Estas tres capas buscan que la responsabilidad no sea solo un documento ético, sino algo vivo en la operación diaria.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 0,
        "seguridad": 100
      },
      "score_final": 5.0,
      "fecha": "2025-11-18T00:49:04.147670Z"
    },
    {
      "id": 44,
      "archivo": "Responsible artificial intelligence governance",
      "pregunta": "¿Cuáles son las condiciones externas e internas que influyen (los 'antecedentes') en cómo una organización aplica la gobernanza de IA responsable?",
      "respuesta_esperada": "El artículo identifica tres grandes fuerzas: Expectativas y normas sociales / regulación: cambios culturales, presión pública, regulación emergente (por ejemplo, leyes sobre IA) que empujan a las empresas a ser responsables. Valores y cultura organizacional: la forma en que la empresa ya toma decisiones, su estructura de poder, su tolerancia ética y su velocidad para adaptarse. Principios de IA responsable que la organización adopta: por ejemplo, transparencia, equidad, privacidad, seguridad, supervisión humana, bienestar social y ambiental. Estas fuerzas determinan qué tan en serio toma la empresa la responsabilidad en IA y cómo la traduce en procesos reales.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:49:17.205451Z"
    },
    {
      "id": 45,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Por qué se dice que es urgente regular la IA a nivel global y no solo a nivel nacional?",
      "respuesta_esperada": "Porque la IA ya está metida en sectores críticos (salud, finanzas, transporte, vigilancia) y puede tomar decisiones autónomas que afectan derechos humanos, privacidad, seguridad y equidad social. Hoy no existe un marco unificado: la Unión Europea tiene protecciones fuertes como el GDPR (incluyendo derechos como el derecho a explicación), mientras que otras regiones no tienen marcos comparables, lo que crea un paisaje regulatorio fragmentado que dificulta cooperación internacional y control ético del impacto de la IA. Esto se agrava por riesgos como sesgos algorítmicos, vigilancia masiva, manipulación política y armas autónomas.",
      "respuesta_recibida": "La urgencia de regular la IA a nivel global, y no solo a nivel nacional, se debe a los riesgos transfronterizos asociados a su uso y a la necesidad de abordar vacíos regulatorios (Documento 2). La ausencia de marcos normativos estandarizados y la escasa armonización internacional comprometen la confianza del usuario y la sostenibilidad del sistema (Documento 2). Además, la necesidad de regulación sólida es innegable para evitar que el desarrollo tecnológico conlleve regresiones sociales, y se corre el peligro de desbordar las capacidades de los sistemas existentes de protección de derechos sin una regulación adecuada (Documento 11). La Cumbre de Acción sobre Inteligencia Artificial en París en febrero de 2025, que reunió a líderes de diversas esferas y abordó el futuro de la IA desde perspectivas éticas, regulatorias y tecnológicas, organizada por el gobierno francés en colaboración con la Comisión Europea y organismos internacionales, evidencia la importancia de un enfoque global (Documento 10).",
      "num_documento": 17,
      "scores": {
        "exactitud": 70,
        "cobertura": 60,
        "claridad": 85,
        "citas": 60,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 70.75,
      "fecha": "2025-11-18T00:49:34.347571Z"
    },
    {
      "id": 46,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Cómo se comparan los enfoques regulatorios de la Unión Europea, Estados Unidos y Asia?",
      "respuesta_esperada": "Unión Europea: adopta un enfoque integral y basado en derechos, con el GDPR y propuestas como el AI Act, que buscan transparencia, responsabilidad y control humano, sobre todo en aplicaciones de 'alto riesgo'. Estados Unidos: sigue un modelo descentralizado y sectorial. Diferentes agencias (por ejemplo, FDA en salud, FAA en aviación, Departamento de Defensa en defensa) emiten guías propias. Esto permite flexibilidad e innovación específica por sector, pero genera vacíos e inconsistencias nacionales. Asia: no hay una sola estrategia. China impulsa una estrategia estatal para ser potencia en IA; Japón enfatiza la integración social responsable (empleo, población envejecida, privacidad); y Corea del Sur invierte en IA ética mediante alianzas público-privadas. Estas diferencias reflejan prioridades distintas: competitividad tecnológica, estabilidad social y ética.",
      "respuesta_recibida": "La Unión Europea aboga por un uso de la inteligencia artificial abierto, sin abusos, con un enfoque ético, democrático, sostenible y que respete los derechos de los ciudadanos, incluyendo la preservación de la veracidad informativa y la protección de los puestos de trabajo, sugiriendo una moratoria reflexiva para evitar daños. Se están tomando diversas iniciativas jurídicas a nivel europeo, como las impulsadas por el Tribunal Europeo de Derechos Humanos y la Agencia de los Derechos Fundamentales de la Unión Europea (2020), la publicación del Libro Blanco sobre la IA (2020), y la entrada en vigor del Reglamento General de Protección de Datos (RGPD, 2021), promovido por la Comisión Europea. A nivel nacional, se han implementado medidas complementarias, como la Estrategia Nacional de IA (ENIA, 2020) y la Ley Orgánica de Protección de Datos y Garantía de los Derechos Digitales. (Documento 2)\n\nDurante la Cumbre de Acción sobre Inteligencia Artificial, se propusieron un marco normativo internacional que incluye la creación de una agencia europea de supervisión de la IA. Se anunciaron inversiones de hasta 200.000 millones de euros destinadas a fortalecer la competitividad y la innovación en el sector. Se hizo hincapié en la importancia de establecer mecanismos de verificación y auditación para los modelos de IA de alto riesgo, promover la formación y garantizar la transparencia de los algoritmos. La \"Declaración de París sobre Inteligencia Artificial\" refleja el compromiso colectivo por un desarrollo seguro, inclusivo y transparente de la IA, pero muestra una clara divergencia en las estrategias regulatorias internacionales al no ser suscrita por Estados Unidos ni Reino Unido. (Documento 1)\n\nEn Estados Unidos, la regulación de la IA se realiza de forma sectorial. Por ejemplo, la Administración de Alimentos y Medicamentos (FDA) trabaja en políticas para dispositivos médicos y atención médica, mientras que la Administración Federal de Aviación regula la IA en la aviación. Este enfoque permite regulaciones adaptadas a los desafíos y requisitos específicos de cada sector. (Documento 3)\n\nLos enfoques regulatorios en Asia varían significativamente entre países, reflejando los diversos paisajes socioeconómicos de la región. China tiene su propia estrategia nacional de IA. (Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 85,
        "claridad": 75,
        "citas": 60,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 77.75,
      "fecha": "2025-11-18T00:49:49.201482Z"
    },
    {
      "id": 47,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Cuáles son los principales riesgos éticos asociados al uso avanzado de IA?",
      "respuesta_esperada": "El documento destaca: Sesgo algorítmico y discriminación: los modelos pueden amplificar prejuicios históricos y afectar decisiones en justicia penal, crédito, reclutamiento o reconocimiento facial, con impacto desigual en mujeres y grupos racializados. Vigilancia masiva y privacidad: sistemas de vigilancia con IA permiten recolectar y analizar datos personales a gran escala, planteando tensiones entre seguridad y libertades civiles. Armas autónomas y toma de decisiones letales sin intervención humana: surgen dudas sobre responsabilidad moral y legal cuando una máquina toma decisiones de vida o muerte. Desplazamiento laboral y desigualdad económica: la automatización de tareas repetitivas puede sacar personas del mercado laboral y obligar a reconversión masiva de habilidades. Los autores dicen que estos riesgos exigen marcos éticos y regulatorios robustos, no solo avances técnicos.",
      "respuesta_recibida": "Los principales riesgos éticos asociados al uso avanzado de IA incluyen:\n\n*   **Manipulación a los usuarios humanos**: La IA tiene el potencial de manipular a los usuarios, y los niños y jóvenes son especialmente susceptibles a esto, pudiendo verse influenciados negativamente por conversaciones inapropiadas (Documento 1).\n*   **Ciberdelincuencia, suplantación de identidades y estafas**: El uso malintencionado de la IA puede derivar en estas actividades (Documento 2).\n*   **Desigualdad social**: El acceso desigual a la IA puede incrementar la desigualdad social, dejando a algunas poblaciones en desventaja (Documento 2).\n*   **Deshumanización de trabajos y pérdida de empleo**: La IA puede llevar a la deshumanización de ciertos trabajos y a la pérdida de empleo, especialmente entre sectores vulnerables (Documento 2).\n*   **Riesgos algorítmicos derivados de sesgos, falta de transparencia o falta de regulación**: Los algoritmos de IA pueden operar con sesgos, carecer de transparencia o implementarse sin un marco regulador adecuado, lo que conlleva riesgos importantes y puede tener consecuencias negativas para la sociedad (Documento 3).",
      "num_documento": 3,
      "scores": {
        "exactitud": 80,
        "cobertura": 70,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 79.5,
      "fecha": "2025-11-18T00:50:03.740791Z"
    },
    {
      "id": 48,
      "archivo": "Shaping the future of AI balancing innovation and ethics in global regulation",
      "pregunta": "¿Qué principios se proponen como base de un marco internacional de regulación de IA responsable?",
      "respuesta_esperada": "El texto sugiere que cualquier marco global debería incluir al menos: Transparencia: entender cómo decide el sistema y garantizar derecho a explicación en decisiones de alto impacto. Rendición de cuentas: definir claramente quién es responsable cuando un sistema de IA causa daño o actúa de forma no ética. Equidad y no discriminación: evitar sesgos y asegurar que el modelo funcione de forma justa entre distintos grupos demográficos. Privacidad y protección de datos: proteger datos personales, exigir consentimiento informado y limitar usos indebidos. Seguridad y robustez: sistemas seguros frente a ciberataques y que no pongan en riesgo a las personas. Respeto a los derechos humanos y beneficio social: alinear la IA con dignidad humana, bienestar social y el interés público global.",
      "respuesta_recibida": "Los principios que se proponen como base de un marco internacional de regulación de IA responsable incluyen el desarrollo seguro, inclusivo y transparente de la IA, así como la transparencia, equidad y trazabilidad. (Documento 1, Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 70,
        "cobertura": 40,
        "claridad": 80,
        "citas": 70,
        "alucinacion": 95,
        "seguridad": 100
      },
      "score_final": 68.25,
      "fecha": "2025-11-18T00:50:16.208888Z"
    },
    {
      "id": 49,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Cómo ha evolucionado la IA desde sistemas basados en reglas hasta inteligencia impulsada por datos?",
      "respuesta_esperada": "Pasó de sistemas simbólicos/experto con reglas explícitas a un cambio de paradigma en los 80s con aprendizaje automático que aprende de datos; luego, en los 2000s, redes neuronales y deep learning impulsados por cómputo y big data lograron avances en visión, NLP y sistemas autónomos; hoy destacan modelos preentrenados a gran escala y la IA generativa, junto con desafíos éticos (sesgo, transparencia, impacto social).",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:50:27.898929Z"
    },
    {
      "id": 50,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Cuáles son tres limitaciones clave de los sistemas basados en reglas que motivaron el giro hacia el aprendizaje con datos?",
      "respuesta_esperada": "a) Inflexibilidad: no se adaptan a situaciones nuevas no contempladas por las reglas. b) Problemas de escalabilidad: el número de reglas crece exponencialmente y se vuelve inmanejable. c) Fragilidad ante la incertidumbre: si la entrada no coincide exactamente con las reglas, fallan en entornos reales.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 10,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 11.5,
      "fecha": "2025-11-18T00:50:40.206947Z"
    },
    {
      "id": 51,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Qué habilitó el auge del deep learning en los 2000s–2010s y qué tipos de redes protagonizaron los avances?",
      "respuesta_esperada": "Lo habilitaron más poder de cómputo (GPUs), grandes conjuntos de datos y técnicas de entrenamiento mejoradas (p. ej., backprop en redes más profundas). Los hitos vinieron con redes convolucionales (CNNs) en visión, RNN/LSTM en secuencias y traducción y la consolidación de aprendizaje profundo como enfoque dominante.",
      "respuesta_recibida": "El auge del deep learning en los 2000s–2010s fue habilitado por la disponibilidad de grandes conjuntos de datos, un aumento en el poder computacional y técnicas más avanzadas (Documento 1). Los tipos de redes que protagonizaron los avances fueron las redes neuronales y el deep learning (Documento 1).",
      "num_documento": 3,
      "scores": {
        "exactitud": 60,
        "cobertura": 50,
        "claridad": 70,
        "citas": 100,
        "alucinacion": 90,
        "seguridad": 100
      },
      "score_final": 71.0,
      "fecha": "2025-11-18T00:50:53.481802Z"
    },
    {
      "id": 52,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Menciona dos tendencias de los 2020s y un ejemplo de aplicación para cada una.",
      "respuesta_esperada": "Generative AI y LLMs: modelos como GPT-3/PaLM/Claude habilitan chatbots, asistentes y creación automática de contenido. IA en salud: diagnóstico por imagen (rayos X, MRI, CT), detección temprana y apoyo a descubrimiento de fármacos (incluido el caso COVID-19).",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 5,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:51:06.568472Z"
    },
    {
      "id": 53,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Qué es el 'boom de la ética de la IA' y qué lo causó?",
      "respuesta_esperada": "El 'boom de la ética de la IA' describe el aumento explosivo de guías, marcos éticos y llamados a regulación alrededor de la IA. Esto ocurre porque el uso de IA creció muy rápido y empezó a generar riesgos reales: violaciones de privacidad, vigilancia masiva, discriminación algorítmica, problemas de seguridad, impactos ambientales y consecuencias no intencionadas que afectan especialmente a poblaciones vulnerables. Frente a eso, gobiernos, empresas, academia y sociedad civil comenzaron a publicar lineamientos éticos para intentar controlar el impacto de la IA.",
      "respuesta_recibida": "El 'boom de la ética de la IA' se refiere a un período marcado por una demanda sin precedentes de regulación y guía normativa en el campo de la inteligencia artificial (IA) (Documento 34, 35). Esto fue causado por los riesgos asociados con la expansión de la industria de la IA, incluyendo riesgos para la privacidad del usuario, el potencial de aumento de la vigilancia, el costo ambiental de la industria y la amplificación a gran escala de prejuicios y discriminación que pueden afectar desproporcionadamente a grupos vulnerables (Documento 34, 35).",
      "num_documento": 45,
      "scores": {
        "exactitud": 95,
        "cobertura": 90,
        "claridad": 90,
        "citas": 70,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 88.75,
      "fecha": "2025-11-18T00:51:24.373640Z"
    },
    {
      "id": 54,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Cuáles son los principios éticos que más se repiten en las guías analizadas a nivel mundial?",
      "respuesta_esperada": "Al analizar 200 lineamientos de gobernanza de IA publicados por gobiernos, empresas privadas, ONGs y universidades en 37 países, se identificaron al menos 17 principios recurrentes. Entre los más citados están: Transparencia / explicabilidad / auditabilidad: que los sistemas sean entendibles y auditables. Justicia / equidad / no discriminación: evitar sesgos y daños desproporcionados a grupos vulnerables. Responsabilidad / rendición de cuentas: que quienes desarrollan y despliegan IA asuman consecuencias. Privacidad / protección de datos: control sobre el uso de la información personal. Confiabilidad / seguridad / robustez / confianza: que los sistemas sean seguros, fiables y aceptables socialmente. Estos principios aparecen de forma consistente en trabajos previos y se confirman también en este análisis ampliado.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 10,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:51:37.522787Z"
    },
    {
      "id": 55,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "Cuáles son las principales brechas y problemas detectados en las guías éticas actuales sobre IA?",
      "respuesta_esperada": "El estudio señala varias alertas: Sesgo geográfico y lingüístico: la mayoría de documentos proviene de Europa, Norteamérica y partes de Asia; regiones como África y América Latina están subrepresentadas, en parte por barreras de idioma y acceso. Poca representación de autoras mujeres: entre los documentos donde hay autores identificados, la mayoría de nombres inferidos son masculinos; además, en 66% de los textos ni siquiera se lista autoría, lo que dificulta evaluar diversidad. Énfasis en principios vagos y 'soft law': la gran mayoría de documentos son recomendaciones no vinculantes y no explican cómo implementar técnicamente esos principios ni cómo fiscalizarlos. Muy pocos proponen regulación legal obligatoria. Falta de atención a ciertos riesgos estructurales: temas como derechos laborales, impacto ambiental, sostenibilidad, desinformación y efectos a largo plazo (por ejemplo, autonomía letal de sistemas o riesgos existenciales) reciben mucha menos atención que transparencia y fairness. Estas brechas muestran que todavía no hay una gobernanza global equilibrada ni realmente aplicada en la práctica.",
      "respuesta_recibida": "No tengo información suficiente.",
      "num_documento": 3,
      "scores": {
        "exactitud": 0,
        "cobertura": 0,
        "claridad": 0,
        "citas": 0,
        "alucinacion": 100,
        "seguridad": 100
      },
      "score_final": 10.0,
      "fecha": "2025-11-18T00:51:49.728424Z"
    },
    {
      "id": 56,
      "archivo": "Worldwide AI ethics: A review of 200 guidelines",
      "pregunta": "¿Por qué la falta de regulación obligatoria en IA se considera un problema en las guías éticas actuales?",
      "respuesta_esperada": "Muchas guías de ética de IA son 'soft law': recomendaciones, principios o marcos voluntarios sin fuerza legal. El problema es que describen valores como transparencia, equidad o protección de datos, pero casi nunca dicen cómo aplicarlos técnicamente ni quién es responsable si no se cumplen. Eso deja vacíos en rendición de cuentas, supervisión y fiscalización real, especialmente en sectores de alto impacto social donde la IA ya está tomando decisiones.",
      "respuesta_recibida": "La falta de un marco regulador en IA empeora la situación, ya que las grandes corporaciones y los estados tienden a inmiscuirse de forma desmesurada en la vida privada del individuo, vulnerando derechos fundamentales cuando no existen límites legales claros ni mecanismos de control y supervisión efectivos. La rápida inmersión de la IA ha puesto de manifiesto esta falta de regulación, dado que la tecnología evoluciona más rápido que la ley. (Documento 3)",
      "num_documento": 3,
      "scores": {
        "exactitud": 75,
        "cobertura": 60,
        "claridad": 90,
        "citas": 50,
        "alucinacion": 80,
        "seguridad": 100
      },
      "score_final": 70.75,
      "fecha": "2025-11-18T00:52:02.076492Z"
    }
  ],
  "resumen": {
    "provider_rag": "gemini",
    "total_preguntas": 56,
    "score_promedio": 59.77,
    "promedios_criterios": {
      "exactitud": 57.59,
      "cobertura": 52.77,
      "claridad": 61.07,
      "citas": 52.86,
      "alucinacion": 86.61,
      "seguridad": 100.0
    }
  }
}